<div class="container">

<table style="width: 100%;"><tr>
<td>Kappa.test</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate Cohen's kappa statistics for agreement</h2>

<h3>Description</h3>

<p>Calculate Cohen's kappa statistics for agreement and its confidence intervals 
followed by testing null-hypothesis that the extent of agreement is same as random,
kappa statistic equals zero.
</p>


<h3>Usage</h3>

<pre><code class="language-R"> Kappa.test(x, y=NULL, conf.level=0.95) </code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>If y is not given, x must be the square matrix that the rows and columns 
show the ratings of different rater (or repeated measure) and the values indicate 
the numbers of data having that combination.  If y is given, x must be the result 
of ratings by the first rater (or first time measurement).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>If given, y must be the result of ratings by the second rater (or second 
time measurement).  As default, it is not given.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf.level</code></td>
<td>
<p>Probability for confidence intervals for kappa statistics.  Default is 0.95.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Result$statistic</code></td>
<td>
<p>Z score to test null-hypothesis.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Result$estimate</code></td>
<td>
<p>Calculated point estimate of Cohen's kappa statistic.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Result$conf.int</code></td>
<td>
<p>A numeric vector of length 2 to give upper/lower limit of 
confidence intervals.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Result$p.value</code></td>
<td>
<p>The significant probability as the result of null-hypothesis testing.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Judgement</code></td>
<td>
<p>The judgement for the estimated kappa about the extent of agreement, 
given by Landis JR, Koch GG (1977) Biometrics, 33: 159-174: If kappa is less than 0, "No 
agreement", if 0-0.2, "Slignt agreement", if 0.2-0.4, "Fair agreement", if 0.4-0.6, 
"Moderate agreement", if 0.6-0.8, "Substantial agreement", if 0.8-1.0, 
"Almost perfect agreement".</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Minato Nakazawa <a href="mailto:minatonakazawa@gmail.com">minatonakazawa@gmail.com</a> <a href="https://minato.sip21c.org/">https://minato.sip21c.org/</a></p>


<h3>References</h3>

<p>Landis JR, Koch GG (1977) The measurement of observer agreement for categorical 
data.  Biometrics, 33: 159-174.
</p>


<h3>See Also</h3>

<p>Kappa</p>


<h3>Examples</h3>

<pre><code class="language-R"> res &lt;- Kappa.test(matrix(c(20, 10, 5, 15), 2, 2))
 str(res)
 print(res)
 Kappa.test(c(1, 1, 3, 1, 1, 2, 1, 2, 1, 1), c(2, 1, 3, 1, 3, 2, 1, 3, 3, 3))
</code></pre>


</div>