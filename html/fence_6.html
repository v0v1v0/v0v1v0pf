<div class="container">

<table style="width: 100%;"><tr>
<td>IF.lm</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Invisible Fence model selection (Linear Model)</h2>

<h3>Description</h3>

<p>Invisible Fence model selection (Linear Model)
</p>


<h3>Usage</h3>

<pre><code class="language-R">IF.lm(full, data, B = 100, cpus = 2, lftype = c("abscoef", "pvalue"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>full</code></td>
<td>
<p>formula of full model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>
<p>number of bootstrap sample, parametric for lm</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cpus</code></td>
<td>
<p>number of parallel computers</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lftype</code></td>
<td>
<p>subtractive measure type, e.g., absolute value of coefficients, p-value, t-value, etc.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This method (Jiang et. al, 2011) is motivated by computational expensive in complex and high dimensional problem.
The idea of the methodâ€“there is the best model in each dimension (in model space).  The boostrapping determines the coverage
probability of the selected model in each dimensions. The parsimonious model is the selected model with the highest coverage probabily
(except the one for the full model, always probability of 1.)
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>full</code></td>
<td>
<p>list the full model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>
<p>list the number of bootstrap samples that have been used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>freq</code></td>
<td>
<p>list the coverage probabilities of the selected model for each dimension</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>size</code></td>
<td>
<p>list the number of variables in the parsimonious model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>term</code></td>
<td>
<p>list variables included in the full model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>list the variables selected in-the-order in the parsimonious model</p>
</td>
</tr>
</table>
<p>@note The current Invisible Fence focuses on variable selection. The current routine is applicable to the case in which
the subtractive measure is the absolute value of the coefficients, p-value, t-value.
However, the method can be extended to other subtractive measures.  See Jiang et. al (2011) for more details.
</p>


<h3>Author(s)</h3>

<p>Jiming Jiang  Jianyang Zhao  J. Sunil Rao  Thuan Nguyen
</p>


<h3>References</h3>


<ul>
<li>
<p>Jiang J., Rao J.S., Gu Z., Nguyen T. (2008),  Fence Methods for Mixed Model Selection. The Annals of Statistics, 36(4): 1669-1692
</p>
</li>
<li>
<p>Jiming Jiang, Thuan Nguyen and J. Sunil Rao (2011), Invisible fence methods and the identification of differentially expressed gene sets. Statistics and Its Interface, Volume 4, 403-415.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">library(fence)
library(MASS)
library(snow)
r =1234; set.seed(r)
p=10; n=300; rho = 0.6
R = diag(p)
for(i in 1:p){
  for(j in 1:p){
     R[i,j] = rho^(abs(i-j))
  }
}
R = 1*R
x=mvrnorm(n, rep(0, p), R)
colnames(x)=paste('x',1:p, sep='')
X = cbind(rep(1,n),x)
tbetas = c(1,1,1,0,1,1,0,1,0,0,0)  # non-zero beta 1,2,4,5,7
epsilon = rnorm(n)
y = as.matrix(X)%*%tbetas + epsilon
colnames(y) = 'y'
data = data.frame(cbind(X,y))
full = y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10
# Takes greater than 5 seconds (~`17 seconds) to run
# obj1 = IF.lm(full = full, data = data, B = 100, lftype = "abscoef")
# sort((names(obj1$model$coef)[-1]))  
# obj2 = IF.lm(full = full, data = data, B = 100, lftype = "pvalue")
# sort(setdiff(names(data[c(-1,-12)]), names(obj2$model$coef)))

</code></pre>


</div>