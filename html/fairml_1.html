<div class="container">

<table style="width: 100%;"><tr>
<td>fairml-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fair models in machine learning</h2>

<h3>Description</h3>

<p>Fair machine learning models: estimation, tuning and prediction.
</p>


<h3>Details</h3>

<p><span class="pkg">fairml</span> implements key algorithms for learning machine learning models
while enforcing fairness with respect to a set of observed sensitive (or
protected) attributes.
</p>
<p>Currently <span class="pkg">fairml</span> implements the following algorithms (references below):
</p>

<ul>
<li> <p><code>nclm()</code>: the non-convex formulation of fair linear regression
model from Komiyama et al. (2018).
</p>
</li>
<li> <p><code>frrm()</code>: the fair (linear) ridge regression model from Scutari,
Panero and Proissl (2022).
</p>
</li>
<li> <p><code>fgrrm()</code>: thefair generalized (linear) ridge regression model
from Scutari, Panero and Proissl (2022), supporting the Gaussian,
binomial, Poisson, multinomial and Cox (proportional hazards) families.
</p>
</li>
<li> <p><code>zlrm()</code>: the fair logistic regression with covariance
constraints from Zafar et al. (2019).
</p>
</li>
<li> <p><code>zlrm()</code>: a fair linear regression with covariance
constraints following Zafar et al. (2019).
</p>
</li>
</ul>
<p>Furthermore, different fairness definitions can be used in <code>frrm()</code>
and <code>fgrrm()</code>:
</p>

<ul>
<li> <p><code>"sp-komiyama"</code>: the statistical parity fairness constraint from
Komiyama et al. (2018);
</p>
</li>
<li> <p><code>"eo-komiyama"</code>: the analogous equality of opportunity constraint
built on the proportion of variance (or deviance) explained by sensitive
attributes;
</p>
</li>
<li> <p><code>"if-berk"</code>: the individual fairness constraint from Berk et al.
(2017) adapted in Scutari, Panero and Proissl (2022);
</p>
</li>
<li>
<p> user-provided functions for custom definitions.
</p>
</li>
</ul>
<p>In addition, <span class="pkg">fairml</span> implements diagnostic plots, cross-validation,
prediction and methods for most of the generics made available for linear
models from <code>lm()</code> and <code>glm()</code>. Profile plots to trace key model
and goodness-of-fit indicators at varying levels of fairness are available
from <br><code>fairness.profile.plot()</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari<br>
Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA)<br></p>
<p>Maintainer: Marco Scutari <a href="mailto:scutari@bnlearn.com">scutari@bnlearn.com</a>
</p>


<h3>References</h3>

<p>Berk R, Heidari H, Jabbari S, Joseph M, Kearns M, Morgenstern J, Neel S,
Roth A (2017). "A Convex Framework for Fair Regression". FATML. <br><code>https://www.fatml.org/media/documents/convex_framework_for_fair_regression.pdf</code>
</p>
<p>Komiyama J, Takeda A, Honda J, Shimao H (2018). "Nonconvex Optimization for
Regression with Fairness Constraints". Proceedings of the 35th International
Conference on Machine Learning (ICML), PMLR <strong>80</strong>:2737–2746. <br><code>http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf</code>
</p>
<p>Scutari M, Panero F, Proissl M (2022). "Achieving Fairness with a Simple Ridge
Penalty". Statistics and Computing, <strong>32</strong>, 77. <br><code>https://link.springer.com/content/pdf/10.1007/s11222-022-10143-w.pdf</code>
</p>
<p>Zafar BJ, Valera I, Gomez-Rodriguez M, Gummadi KP (2019). "Fairness
Constraints: a Flexible Approach for Fair Classification". Journal of
Machine Learning Research, 30:1–42. <br><code>https://www.jmlr.org/papers/volume20/18-262/18-262.pdf</code>
</p>


</div>