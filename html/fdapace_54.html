<div class="container">

<table style="width: 100%;"><tr>
<td>MultiFAM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Functional Additive Models with Multiple Predictor Processes</h2>

<h3>Description</h3>

<p>Smooth backfitting procedure for functional additive models with multiple predictor processes
</p>


<h3>Usage</h3>

<pre><code class="language-R">MultiFAM(
  Y,
  X,
  ker = "epan",
  nEval = 51,
  XTest = NULL,
  bwMethod = 0,
  alpha = 0.7,
  supp = c(-2, 2),
  optnsList = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>An <em>n</em>-dimensional vector whose elements consist of scalar responses.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>A <em>d</em>-dimensional list whose components consist of two lists of <em>Ly</em> and <em>Lt</em> containing observation times and functional covariate values for each predictor component, respectively. For details of <em>Ly</em> and <em>Lt</em>, see <code>FPCA</code> for detail.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ker</code></td>
<td>
<p>A <code>function</code> object representing the base kernel to be used in the smooth backfitting algorithm (default is 'epan' which is the only option supported currently).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nEval</code></td>
<td>
<p>The number of evaluation grid points for kernel smoothing (default is 51. If it is specified as 0, then estimated FPC scores in the training set are used for evaluation grid instead of equal grid).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>XTest</code></td>
<td>
<p>A <em>d</em>-dimensional list for test set of functional predictors (default is NULL). If <code>XTest</code> is specified, then estimated FPC scores in the test set are used for evaluation grid.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bwMethod</code></td>
<td>
<p>The method of initial bandwidth selection for kernel smoothing, a positive value for designating K-fold cross-validtaion and zero for GCV (default is 0)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The shrinkage factor (positive number) for bandwidth selection. See Han et al. (2016) (default is 0.7).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>supp</code></td>
<td>
<p>The lower and upper limits of kernel smoothing domain for studentized FPC scores, which FPC scores are divided by the square roots of eigenvalues (default is [-2,2]).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optnsList</code></td>
<td>
<p>A <em>d</em>-dimensional list whose components consist of <code>optns</code> for each predictor component, respectively. (default is NULL which assigns the same default <code>optns</code> for all components as in <code>FPCA</code>).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>MultiFAM</code> fits functional additive models for a scalar response and 
multiple predictor processes and implements  the smooth backfitting  algorithm provided in
Han, K., Müller, H.G., Park, B.U.  (2018). Smooth backfitting for additive modeling with small errors-in-variables, 
with an application to additive functional regression for multiple predictor functions. Bernoulli 24, 1233–1265.
</p>
<p>It is based on the model  </p>
<p style="text-align: center;"><code class="reqn">E(Y | \mathbf{X}) = \sum_{j=1}^d \sum_{k=1}^{K_j} g_{jk}(\xi_{jk}),</code>
</p>
<p> where <code class="reqn">\xi_{jk}</code> stand for the k-th FPC scores of the j-th predictor 
process. <code>MultiFAM</code> only is for the multiple predictor processes case.
For a univariate predictor use FAM, the functional additive model (Müller and Yao 2008). 
It is necessary to designate an estimation support for the additive component functions where the additive modeling is only allowed over 
restricted intervals  (see Han et al., 2018).
</p>


<h3>Value</h3>

<p>A list containing the following fields:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>A scalar for the centered regression model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>SBFit</code></td>
<td>
<p>An <em>N</em> by <code class="reqn">(K_1 + ... + K_d)</code> matrix whose column vectors consist of the smooth backfitting component 
function estimators at the given <em>N</em> estimation points.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xi</code></td>
<td>
<p>An <em>N</em> by <code class="reqn">(K_1 + ... + K_d)</code> matrix whose column vectors consist of the FPC score grid vectors 
at which each additive component function is evaluated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bw</code></td>
<td>
<p>A <code class="reqn">(K_1 + ... + K_d)</code>-dimensional bandwidth vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>A <code class="reqn">(K_1 + ... + K_d)</code>-dimensional vector containing eigenvalues.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>phi</code></td>
<td>
<p>A <em>d</em>-dimensional list whose components consist of an <em>nWorkGrid</em> by <em>K_j</em> matrix containing eigenfunctions, 
supported by <code>WorkGrid</code>. See <code>FPCA</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>workGrid</code></td>
<td>
<p>A <em>d</em>-dimensional list whose components consist of an <em>nWorkGrid</em> by <em>K_j</em> working grid, 
a regular grid on which the eigenanalysis is carried out See <code>FPCA</code>.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p><cite>Mammen, E., Linton, O. and Nielsen, J. (1999), "The existence and asymptotic properties of a backfitting projection algorithm under weak conditions", Annals of Statistics, Vol.27, No.5, p.1443-1490.</cite>
</p>
<p><cite>Mammen, E. and Park, B. U. (2006), "A simple smooth backfitting method for additive models", Annals of Statistics, Vol.34, No.5, p.2252-2271.</cite>
</p>
<p><cite>Müller, H.-G. and Yao, F. (2008), "Functional additive models", Journal of the American Statistical Association, Vol.103, No.484, p.1534-1544.</cite>
</p>
<p><cite>Han, K., Müller, H.-G. and Park, B. U. (2016), "Smooth backfitting for additive modeling with small errors-in-variables, with an application to additive functional regression for multiple predictor functions", Bernoulli (accepted).</cite>
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(1000)

library(MASS)

f11 &lt;- function(t) t
f12 &lt;- function(t) 2*cos(2*pi*t/4)
f21 &lt;- function(t) 1.5*sin(2*pi*t/4)
f22 &lt;- function(t) 1.5*atan(2*pi*t/4)

n&lt;-100
N&lt;-200

sig &lt;- matrix(c(2.0, 0.0, 0.5, -.2,
                0.0, 1.2, -.2, 0.3,
                0.5, -.2, 1.7, 0.0,
                -.2, 0.3, 0.0, 1.0),
              nrow=4,ncol=4)

scoreX &lt;- mvrnorm(n,mu=rep(0,4),Sigma=sig)
scoreXTest &lt;- mvrnorm(N,mu=rep(0,4),Sigma=sig)

Y &lt;- f11(scoreX[,1]) + f12(scoreX[,2]) + f21(scoreX[,3]) + f22(scoreX[,4]) + rnorm(n,0,0.5)
YTest &lt;- f11(scoreXTest[,1]) + f12(scoreXTest[,2]) + 
f21(scoreXTest[,3]) + f22(scoreXTest[,4]) + rnorm(N,0,0.5)

phi11 &lt;- function(t) sqrt(2)*sin(2*pi*t)
phi12 &lt;- function(t) sqrt(2)*sin(4*pi*t)
phi21 &lt;- function(t) sqrt(2)*cos(2*pi*t)
phi22 &lt;- function(t) sqrt(2)*cos(4*pi*t)

grid &lt;- seq(0,1,length.out=21)
Lt &lt;- Lx1 &lt;- Lx2 &lt;- list()
for (i in 1:n) {
  Lt[[i]] &lt;- grid
  Lx1[[i]] &lt;- scoreX[i,1]*phi11(grid) + scoreX[i,2]*phi12(grid) + rnorm(1,0,0.01)
  Lx2[[i]] &lt;- scoreX[i,3]*phi21(grid) + scoreX[i,4]*phi22(grid) + rnorm(1,0,0.01)
}

LtTest &lt;- Lx1Test &lt;- Lx2Test &lt;- list()
for (i in 1:N) {
  LtTest[[i]] &lt;- grid
  Lx1Test[[i]] &lt;- scoreXTest[i,1]*phi11(grid) + scoreXTest[i,2]*phi12(grid) + rnorm(1,0,0.01)
  Lx2Test[[i]] &lt;- scoreXTest[i,3]*phi21(grid) + scoreXTest[i,4]*phi22(grid) + rnorm(1,0,0.01)
}

X1 &lt;- list(Ly=Lx1, Lt=Lt)
X2 &lt;- list(Ly=Lx2, Lt=Lt)

X1Test &lt;- list(Ly=Lx1Test, Lt=LtTest)
X2Test &lt;- list(Ly=Lx2Test, Lt=LtTest)

X &lt;- list(X1, X2)
XTest &lt;- list(X1Test, X2Test)

# estimation
sbf &lt;- MultiFAM(Y=Y,X=X)

xi &lt;- sbf$xi

par(mfrow=c(2,2))
j &lt;- 1
p0 &lt;- trapzRcpp(sort(xi[,j]),dnorm(sort(xi[,j]),0,sqrt(sig[j,j])))
g11 &lt;- f11(sort(xi[,j])) - 
trapzRcpp(sort(xi[,j]),f11(sort(xi[,j]))*dnorm(sort(xi[,j]),0,sqrt(sig[j,j])))/p0
tmpSgn &lt;- sign(sum(g11*sbf$SBFit[,j]))
plot(sort(xi[,j]),g11,type='l',col=2,ylim=c(-2.5,2.5),xlab='xi11')
points(sort(xi[,j]),tmpSgn*sbf$SBFit[order(xi[,j]),j],type='l')
legend('top',c('true','SBF'),col=c(2,1),lwd=2,bty='n',horiz=TRUE)

j &lt;- 2
p0 &lt;- trapzRcpp(sort(xi[,j]),dnorm(sort(xi[,j]),0,sqrt(sig[j,j])))
g12 &lt;- f12(sort(xi[,j])) - 
trapzRcpp(sort(xi[,j]),f12(sort(xi[,j]))*dnorm(sort(xi[,j]),0,sqrt(sig[j,j])))/p0
tmpSgn &lt;- sign(sum(g12*sbf$SBFit[,j]))
plot(sort(xi[,j]),g12,type='l',col=2,ylim=c(-2.5,2.5),xlab='xi12')
points(sort(xi[,j]),tmpSgn*sbf$SBFit[order(xi[,j]),j],type='l')
legend('top',c('true','SBF'),col=c(2,1),lwd=2,bty='n',horiz=TRUE)

j &lt;- 3
p0 &lt;- trapzRcpp(sort(xi[,j]),dnorm(sort(xi[,j]),0,sqrt(sig[j,j])))
g21 &lt;- f21(sort(xi[,j])) - 
trapzRcpp(sort(xi[,j]),f21(sort(xi[,j]))*dnorm(sort(xi[,j]),0,sqrt(sig[j,j])))/p0
tmpSgn &lt;- sign(sum(g21*sbf$SBFit[,j]))
plot(sort(xi[,j]),g21,type='l',col=2,ylim=c(-2.5,2.5),xlab='xi21')
points(sort(xi[,j]),tmpSgn*sbf$SBFit[order(xi[,j]),j],type='l')
legend('top',c('true','SBF'),col=c(2,1),lwd=2,bty='n',horiz=TRUE)

j &lt;- 4
p0 &lt;- trapzRcpp(sort(xi[,j]),dnorm(sort(xi[,j]),0,sqrt(sig[j,j])))
g22 &lt;- f22(sort(xi[,j])) - 
trapzRcpp(sort(xi[,j]),f22(sort(xi[,j]))*dnorm(sort(xi[,j]),0,sqrt(sig[j,j])))/p0
tmpSgn &lt;- sign(sum(g22*sbf$SBFit[,j]))
plot(sort(xi[,j]),g22,type='l',col=2,ylim=c(-2.5,2.5),xlab='xi22')
points(sort(xi[,j]),tmpSgn*sbf$SBFit[order(xi[,j]),j],type='l')
legend('top',c('true','SBF'),col=c(2,1),lwd=2,bty='n',horiz=TRUE)


# fitting
sbf &lt;- MultiFAM(Y=Y,X=X,nEval=0)
yHat &lt;- sbf$mu+apply(sbf$SBFit,1,'sum')
plot(yHat,Y)
abline(coef=c(0,1),col=2)


# R^2
R2 &lt;- 1-sum((Y-yHat)^2)/sum((Y-mean(Y))^2)
R2


# prediction
sbf &lt;- MultiFAM(Y=Y,X=X,XTest=XTest)
yHat &lt;- sbf$mu+apply(sbf$SBFit,1,'sum')
plot(yHat,YTest)
abline(coef=c(0,1),col=2)

</code></pre>


</div>