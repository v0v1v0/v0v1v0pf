<div class="container">

<table style="width: 100%;"><tr>
<td>perf.fbroc.roc</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate performance for bootstrapped ROC curve</h2>

<h3>Description</h3>

<p>Calculates different performance metric for ROC curves based on the bootstrap
results saved in an object of class <code>fbroc.roc</code>. Confidence intervals
are included.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'fbroc.roc'
perf(roc, metric = "auc", conf.level = 0.95,
  tpr = NULL, fpr = NULL, correct.partial.auc = TRUE,
  show.partial.auc.warning = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>roc</code></td>
<td>
<p>An object of class <code>fbroc.roc</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>
<p>A performance metric. Select "auc" for the AUC, "partial.auc" for the partial AUC, 
"tpr" for the TPR at a fixed FPR and "fpr" for the FPR at a fixed TPR.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf.level</code></td>
<td>
<p>The confidence level of the confidence interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tpr</code></td>
<td>
<p>The fixed TPR at which the FPR is to be evaluated when <code>fpr</code> is selected as metric.
If partial AUC is investigated, then an TPR interval over which the partial area is to be calculated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fpr</code></td>
<td>
<p>The fixed FPR at which the TPR is to be evaluated when <code>tpr</code> is selected as metric.
If partial AUC is investigated, then an FPR interval over which the partial area is to be calculated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>correct.partial.auc</code></td>
<td>
<p>Corrects partial AUC for easier interpretation using McClish correction.
Details are given below. Defaults to TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show.partial.auc.warning</code></td>
<td>
<p>Whether to give warnings for partial AUCs below 0.5. Defaults to
true.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Further arguments, that are not used at this time.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list of class <code>fbroc.perf</code>, containing the elements:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Observed.Performance</code></td>
<td>
<p>The observed performance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CI.Performance</code></td>
<td>
<p>Quantile based confidence interval for the performance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf.level</code></td>
<td>
<p>Confidence level of the confidence interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>
<p>Used performance metric.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>
<p>Parameters used to further specifiy metric, e.g. fixed TPR.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.boot</code></td>
<td>
<p>Number of bootstrap replicates used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>boot.results</code></td>
<td>
<p>Performance in each bootstrap replicate.</p>
</td>
</tr>
</table>
<h3>Note on partial AUC correction</h3>

<p>The partial AUC is hard to interpret without considering the range on which it is calculated.
Not only does the partial AUC scale with the width of the interval over which it is calculated,
but it also depends on where the interval is located.
For example, if the ROC Curve is integrated over the FPR interval [0, 0.1] a completely random
and non-discrimate classifier would have a partial AUC of 0.05, but the same ROC curve integrated over
the interval [0.9, 1] would yield a partial AUC of 0.95.
</p>
<p>The correction by McClish produces a corrected partial AUC given by:
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{2} \Big(1 + \frac{\textrm{partialAUC} - \textrm{auc.min}}{\textrm{auc.max} 
- \textrm{auc.min}}\Big)</code>
</p>

<p>Here auc.min is the AUC achieved by the non-discriminate classifier and auc.max is the AUC
achieved by a perfect classifier. Thus, a non-discriminative classifier will always have an AUC
of 0.5 and a perfect one classifier will always have a partial AUCs of 1. 
</p>
<p>Unfortunately, the corrected partial AUC cannot be interpreted in a meaningful way if the curve
is below the non-discriminate classifier, producing corrected partial AUCs values below 0.5. 
For this reason, fbroc will give a warning if the bootstrap produces corrected 
partial AUC values below 0.5.
</p>


<h3>References</h3>

<p>Donna Katzman McClish. (1989). <em>Analyzing a Portion of the ROC Curve.</em>
Medical Decision Making, <a href="http://mdm.sagepub.com/content/9/3/190.abstract">http://mdm.sagepub.com/content/9/3/190.abstract</a>.
</p>


<h3>See Also</h3>

<p><code>boot.roc</code>, <code>print.fbroc.perf</code>, 
<code>plot.fbroc.perf</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">y &lt;- rep(c(TRUE, FALSE), each = 100)
x &lt;- rnorm(200) + y
result.boot &lt;- boot.roc(x, y, n.boot = 100)
perf(result.boot, "auc")
perf(result.boot, "auc", conf.level = 0.99)
perf(result.boot, "partial.auc", fpr = c(0, 0.25), show.partial.auc.warning = FALSE)
</code></pre>


</div>