<div class="container">

<table style="width: 100%;"><tr>
<td>fitmixture</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Estimating parameters of the well-known mixture models</h2>

<h3>Description</h3>

<p>Estimates parameters of the mixture model using the expectation maximization (EM) algorithm. General form for the cdf of a statistical mixture model is given by
</p>
<p style="text-align: center;"><code class="reqn">F(x,{\Theta}) = \sum_{j=1}^{K}\omega_j F_j(x,\theta_j),</code>
</p>

<p>where <code class="reqn">\Theta=(\theta_1,\dots,\theta_K)^T</code>, is the whole parameter vector, <code class="reqn">\theta_j</code> for <code class="reqn">j=1,\dots,K</code> is the parameter space of the <code class="reqn">j</code>-th component, i.e. <code class="reqn">\theta_j=(\alpha_j,\beta_j)^{T}</code>, <code class="reqn">F_j(.,\theta_j)</code> is the cdf of the <code class="reqn">j</code>-th component, and known constant <code class="reqn">K</code> is the number of components. Parameters <code class="reqn">\alpha</code> and <code class="reqn">\beta</code> are the shape and scale parameters or both are the shape parameters. In the latter case, the parameters <code class="reqn">\alpha</code> and <code class="reqn">\beta</code> are called the first and second shape parameters, respectively. We note that the constants <code class="reqn">\omega_j</code>s sum to one, i.e. <code class="reqn">\sum_{j=1}^{K}\omega_j=1</code>. The families considered for the cdf <code class="reqn">F</code> include Birnbaum-Saunders, Burr type XII, Chen, F, Frechet, Gamma, Gompertz, Log-normal, Log-logistic, Lomax, skew-normal, and Weibull.</p>


<h3>Usage</h3>

<pre><code class="language-R">fitmixture(data, family, K, initial=FALSE, starts)</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Vector of observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>Name of the family including: "<code>birnbaum-saunders</code>", "<code>burrxii</code>", "<code>chen</code>", "<code>f</code>", "<code>Frechet</code>", "<code>gamma</code>", "<code>gompetrz</code>", "<code>log-normal</code>", "<code>log-logistic</code>", "<code>lomax</code>", "<code>skew-normal</code>", and "<code>weibull</code>".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>Number of components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial</code></td>
<td>
<p>The sequence of initial values including <code class="reqn">\omega_1,\dots,\omega_K,\alpha_1,\dots,\alpha_K,\beta_1,\dots,\beta_K</code>. For skew normal case the vector of initial values of skewness parameters will be added. By default the initial values automatically is determind by k-means method of clustering.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>starts</code></td>
<td>
<p>If <code>initial=TRUE</code>, then sequence of the initial values must be given.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>It is worth noting that identifiability of the mixture models supposed to be held. For skew-normal case we have <code class="reqn">\theta_j=(\alpha_j,\beta_j,\lambda_j)^{T}</code> in which <code class="reqn">-\infty&lt;\alpha_j&lt;\infty</code>, <code class="reqn">\beta_j&gt;0</code>, and <code class="reqn">-\infty&lt;\lambda_j&lt;\infty</code>, respectively, are the location, scale, and skewness parameters of the <code class="reqn">j</code>-th component, see Azzalini (1985).</p>


<h3>Value</h3>


<ol>
<li>
<p> The output has three parts, The first part includes vector of estimated weight, shape, and scale parameters.
</p>
</li>
<li>
<p> The second part involves a sequence of goodness-of-fit measures consist of Akaike Information Criterion (<code>AIC</code>), Consistent Akaike Information Criterion (<code>CAIC</code>), Bayesian Information Criterion (<code>BIC</code>), Hannan-Quinn information criterion (<code>HQIC</code>), Anderson-Darling (<code>AD</code>), Cram\'eer-von Misses (<code>CVM</code>), Kolmogorov-Smirnov (<code>KS</code>), and log-likelihood (<code>log-likelihood</code>) statistics.
</p>
</li>
<li>
<p> The last part of the output contains clustering vector.</p>
</li>
</ol>
<h3>Author(s)</h3>

<p>Mahdi Teimouri</p>


<h3>References</h3>

<p>A. Azzalini, 1985. A class of distributions which includes the normal ones, <em>Scandinavian Journal of Statistics</em>, 12, 171-178.
</p>
<p>A. P. Dempster, N. M. Laird, and D. B. Rubin, 1977. Maximum likelihood from incomplete data via the EM algorithm, <em>Journal of the Royal Statistical Society Series B</em>, 39, 1-38.
</p>
<p>M. Teimouri, S. Rezakhah, and A. Mohammdpour, 2018. EM algorithm for symmetric stable mixture model, <em>Communications in Statistics-Simulation and Computation</em>, 47(2), 582-604.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Here we model the northern hardwood uneven-age forest data (HW$DIA) in inches using a
# 3-component Weibull mixture distribution.
data(HW)
data&lt;-HW$DIA
K&lt;-3
fitmixture(data,"weibull", K, initial=FALSE)
</code></pre>


</div>