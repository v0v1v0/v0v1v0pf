<div class="container">

<table style="width: 100%;"><tr>
<td>fk_mdh</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Minimum density hyperplanes</h2>

<h3>Description</h3>

<p>Estimates minimum density hyperplanes for clustering using projection pursuit, based on the method of Pavlidis et al. (2016)
</p>


<h3>Usage</h3>

<pre><code class="language-R">fk_mdh(X, v0 = NULL, hmult = 1, beta = c(.25,.25), alphamax = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>numeric data matrix (num_data x num_dimensions).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v0</code></td>
<td>
<p>(optional) vector representing the initial projection direction. Default is the first principal direction.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hmult</code></td>
<td>
<p>(optional) positive numeric. The bandwidth in the kernel density is set to hmult multiplied by Silverman's rule of thumb value,
which is based on the AMISE minimiser when the underlying distribution is Gaussian.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>(optional) numeric vector of kernel coefficients. The default is the smooth order one kernel described by Hofmeyr (2019).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alphamax</code></td>
<td>
<p>(optional) maximum/final (scaled) distance of the optimal hyperplane from the mean of the data.
The default is alphamax = 1.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A named list with fields
</p>
<table>
<tr style="vertical-align: top;">
<td><code>$v</code></td>
<td>
<p>the optimal projection vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$b</code></td>
<td>
<p>the location of the minimum density hyperplane orthogonal to v.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Pavlidis N.G., Hofmeyr D.P., Tasoulis S.K. (2016) "Minimum Density Hyperplanes", <em>Journal of Machine Learning Research</em>, 17(156), 1â€“33.
</p>
<p>Hofmeyr, D.P. (2021) "Fast exact evaluation of univariate kernel sums", <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 43(2), 447-458.
</p>


<h3>Examples</h3>

<pre><code class="language-R">op &lt;- par(no.readonly = TRUE)

set.seed(1)

### Generate data from a simple 10 component mixture model in 10 dimensions:
### Determine means of components

mu &lt;- matrix(runif(100), ncol = 10)

### Determine scales of components (diagonal elements of covariance)

covs &lt;- matrix(rexp(100), ncol = 10)/10

### Determine cluster indicator matrix

I &lt;- t(rmultinom(2000, 1, 1:10))

### Determine mean and residual matrix

M &lt;- I%*%mu

R &lt;- matrix(rnorm(20000), 2000, 10)*(I%*%covs)

### Data is given by the sum of these

X &lt;- M + R

### Find the minimum density hyperplane separator and plot
### the projected data as well as the PCA plot for comparison

mdh &lt;- fk_mdh(X)

par(mfrow = c(2, 2))

plot(X%*%mdh$v, X%*%eigen(cov(X))$vectors[,2], xlab = 'mdh vector',
    ylab = '2nd principal component', main = 'MDH solution')
abline(v = mdh$b, col = 2)
plot(X%*%eigen(cov(X))$vectors[,1:2], xlab = '1st principal component',
    ylab = '2nd principal component', main = 'PCA plot')

plot(fk_density(X%*%mdh$v))
abline(v = mdh$b, col = 2)
plot(fk_density(X%*%eigen(cov(X))$vectors[,1]))

par(op)
</code></pre>


</div>