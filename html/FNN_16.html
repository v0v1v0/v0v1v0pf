<div class="container">

<table style="width: 100%;"><tr>
<td>mutinfo</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Mutual Information</h2>

<h3>Description</h3>

<p>KNN Mutual Information Estimators.</p>


<h3>Usage</h3>

<pre><code class="language-R">  mutinfo(X, Y, k=10, direct=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>an input data matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>an input data matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>the maximum number of nearest neighbors to search. The default value is set to 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>direct</code></td>
<td>
<p>Directly compute or via entropies.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The direct computation is based on the first estimator of A. Kraskov, H. Stogbauer and P.Grassberger (2004) and 
the indirect computation is done via entropy estimates, i.e., I(X, Y) = H (X) + H(Y) - H(X, Y).
The direct method has smaller bias and variance but the indirect method is faster, see Evans (2008).
</p>


<h3>Value</h3>

<p>For direct method,  one mutual information estimate;
For indirect method,a vector of length <code>k</code> for mutual information estimates using <code>1:k</code> nearest neighbors, respectively.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>A. Kraskov, H. Stogbauer and P.Grassberger (2004).
“Estimating mutual information”.
<em>Physical Review E</em>, <b>69</b>:066138, 1–16.
</p>
<p>D. Evans (2008).
“A Computationally efficient estimator for mutual information”.
<em>Proc. R. Soc. A</em>, <b>464</b>, 1203–1215.
</p>


</div>