<div class="container">

<table style="width: 100%;"><tr>
<td>PVS.fit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Impact point selection with PVS
</h2>

<h3>Description</h3>

<p>This function implements the Partitioning Variable Selection (PVS) algorithm. This algorithm is specifically designed for estimating multivarite linear models, where the scalar covariates are derived from the discretisation of a curve.
</p>
<p>PVS is a two-stage procedure that selects the impact points of the discretised curve and estimates the model. The algorithm employs a penalised least-squares regularisation procedure. Additionally, it utilises an objective criterion (<code>criterion</code>) to determine the initial number of covariates in the reduced model (<code>w.opt</code>) of the first stage, and the penalisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class="language-R">PVS.fit(z, y, train.1 = NULL, train.2 = NULL, lambda.min = NULL, 
lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1, nlambda = 100, 
vn = ncol(z), nfolds = 10, seed = 123, wn = c(10, 15, 20), range.grid = NULL, 
criterion = "GCV", penalty = "grSCAD", max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (linear component).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train.1</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 1st step. The default setting is  <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train.2</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 2nd step. The default setting is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>

<p>Number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates in the reduced model. For more information, refer to the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameters: <code>wn.opt</code> and <code>lambda.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only "grLasso" and "grSCAD" are implemented. The default is "grSCAD".
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The sparse linear model with covariates coming from the discretization of a curve is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+\varepsilon_i,\ \ \ (i=1,\dots,n)</code>
</p>

<p>where 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and  <code class="reqn">\zeta_i</code> is assumed to be a random curve defined on some interval <code class="reqn">[a,b]</code>, which is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients.
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li>
</ul>
<p>In  this model, it is assumed that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code> are part of the model. Therefore, the relevant variables (the impact points of the curve <code class="reqn">\zeta</code> on the response) must be selected, and the model estimated.
</p>
<p>In this function, this model is fitted using the PVS. The PVS is a two-steps procedure. So we divide the sample into two independent subsamples, each asymptotically half the size of the original sample (<code class="reqn">n_1\sim n_2\sim n/2</code>). One subsample is used in the first stage of the method, and the other in the second stage.The subsamples are defined as follows:
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified to the program through the arguments <code>train.1</code> and <code>train.2</code>. The superscript <code class="reqn">\mathbf{s}</code>, where <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code>, indicates the stage of the method in which the sample, function, variable, or parameter is involved.
</p>
<p>To explain the algorithm, we assume that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code>, with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> being integers.
</p>

<ol>
<li> <p><b>First step</b>. A reduced model is considered, discarding many linear covariates. The penalised least-squares procedure is applied to the reduced model using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code>. Specifically:
</p>

<ul>
<li>
<p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates, containing only <code class="reqn">w_n</code> equally spaced discretized observations of <code class="reqn">\zeta</code> covering the interval <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>. The size (cardinality) of this subset is provided to the program in the argument <code>wn</code> (which contains a sequence of eligible sizes).
</p>
</li>
<li>
<p>  Consider the following reduced model involving only the <code class="reqn">w_n</code> linear covariates from <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>: <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure is applied to the reduced model using the function <code>lm.pels.fit</code>, which requires the remaining arguments (for details, see the documentation of the function <code>lm.pels.fit</code>). The estimates obtained are the outputs of the first step of the algorithm.
</p>
</li>
</ul>
</li>
<li> <p><b>Second step</b>. The variables selected in the first step, along with the variables in their neighborhood, are included. Then the penalised least-squares procedure is carried out again considering only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li>
<p> Consider a new set of variables :
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, we can rename the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li>
<p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalised least-squares variable selection procedure is applied to this model using <code>lm.pels.fit</code>. 
</p>
</li>
</ul>
</li>
</ol>
<p>The outputs of the second step are the estimates of the model. For further details on this algorithm, see Aneiros and Vieu (2014).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> is not met (then <code class="reqn">p_n/w_n</code> is not an integer), the function considers variable <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>The matched call.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i. e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code> and <code>lambda.opt</code> are used).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w.opt</code></td>
<td>
<p>Selected size for  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code> and <code>lambda.opt</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G. and Vieu, P. (2014) Variable selection in infinite-dimensional problems. <em>Statistics  &amp; Probability Letters</em>, <b>94</b>, 12â€“20, <a href="https://doi.org/10.1016/j.spl.2014.06.025">doi:10.1016/j.spl.2014.06.025</a>.
</p>


<h3>See Also</h3>

<p>See also <code>lm.pels.fit</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(Sugar)

y&lt;-Sugar$ash
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- PVS.fit(z=z.sug[train,], y=y.sug[train],train.1=1:108,train.2=109:216,
        lambda.min.h=0.2,criterion="BIC", max.iter=5000)
proc.time()-ptm

fit 
names(fit)
</code></pre>


</div>