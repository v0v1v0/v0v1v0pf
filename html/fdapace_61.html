<div class="container">

<table style="width: 100%;"><tr>
<td>SBFitting</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Iterative Smooth Backfitting Algorithm</h2>

<h3>Description</h3>

<p>Smooth backfitting procedure for nonparametric additive models
</p>


<h3>Usage</h3>

<pre><code class="language-R">SBFitting(Y, x, X, h = NULL, K = "epan", supp = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>An <em>n</em>-dimensional vector whose elements consist of scalar responses.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>An <em>N</em> by <em>d</em> matrix whose column vectors consist of <em>N</em> vectors of estimation points for each component function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>An <em>n</em> by <em>d</em> matrix whose row vectors consist of multivariate predictors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>h</code></td>
<td>
<p>A <em>d</em> vector of bandwidths for kernel smoothing to estimate each component function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>A <code>function</code> object representing the kernel to be used in the smooth backfitting (default is 'epan', the the Epanechnikov kernel.).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>supp</code></td>
<td>
<p>A <em>d</em> by 2 matrix whose row vectors consist of the lower and upper limits of estimation intervals for each component function (default is the <em>d</em>-dimensional unit rectangle of <em>[0,1]</em>).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>SBFitting</code> fits component functions of additive models for a scalar response and a multivariate predictor based on the 
smooth backfitting algorithm proposed by Mammen et al. (1999), see also Mammen and Park (2006), Yu et al. (2008), Lee et al. (2010, 2012) and 
others. <code>SBFitting</code> only focuses on the locally constant smooth backfitting estimator for the multivariate predictor case. 
Note that the fitting in the special case of a univariate predictor is the same as that provided by a local constant kernel regression 
estimator (Nadaraya-Watson estimator). The local polynomial approach can be extended similarly (currently omitted). 
Support of the multivariate predictor is assumed to be a product of closed intervals. Users should designate an estimation support for the additive
component function where modeling is  restricted to subintervals of the domain  (see Han et al., 2016). If 
one puts <code>X</code> in the argument for the estimation points <code>x</code>, <code>SBFitting</code> returns the estimated values of 
the conditional mean responses given the observed predictors.
</p>


<h3>Value</h3>

<p>A list containing the following fields:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>SBFit</code></td>
<td>
<p>An <em>N</em> by <em>d</em> matrix whose column vectors consist of the smooth backfitting component function estimators at the given estimation points.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mY</code></td>
<td>
<p>A scalar of centered part of the regression model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>NW</code></td>
<td>
<p>An <em>N</em> by <em>d</em> matrix whose column vectors consist of the Nadaraya-Watson marginal regression function estimators for each predictor 
component at the given estimation points.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mgnDens</code></td>
<td>
<p>An <em>N</em> by <em>d</em> matrix whose column vectors consist of the marginal kernel density estimators for 
each predictor component at the given estimation points.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jntDens</code></td>
<td>
<p>An <em>N</em> by <em>N</em> by <em>d</em> by <em>d</em> array representing the 2-dimensional joint kernel 
density estimators for all pairs of predictor components at the given estimation grid. For example, <code>[,,j,k]</code> of 
the object provides the 2-dimensional joint kernel density estimator of the <code>(j,k)</code>-component of predictor components 
at the corresponding <em>N</em> by <em>N</em> matrix of estimation grid.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>itemNum</code></td>
<td>
<p>The iteration number that the smooth backfitting algorithm has stopped.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>itemErr</code></td>
<td>
<p>The iteration error of the smooth backfitting algorithm that represents the maximum L2 distance among component functions in the last successive updates.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p><cite>Mammen, E., Linton, O. and Nielsen, J. (1999), "The existence and asymptotic properties of a backfitting projection algorithm under weak conditions", Annals of Statistics, Vol.27, No.5, p.1443-1490.</cite>
</p>
<p><cite>Mammen, E. and Park, B. U. (2006), "A simple smooth backfitting method for additive models", Annals of Statistics, Vol.34, No.5, p.2252-2271.</cite>
</p>
<p><cite>Yu, K., Park, B. U. and Mammen, E. (2008), "Smooth backfitting in generalized additive models", Annals of Statistics, Vol.36, No.1, p.228-260.</cite>
</p>
<p><cite>Lee, Y. K., Mammen, E. and Park., B. U. (2010), "backfitting and smooth backfitting for additive quantile models", Vol.38, No.5, p.2857-2883.</cite>
</p>
<p><cite>Lee, Y. K., Mammen, E. and Park., B. U. (2012), "Flexible generalized varying coefficient regression models", Annals of Statistics, Vol.40, No.3, p.1906-1933.</cite>
</p>
<p><cite>Han, K., MÃ¼ller, H.-G. and Park, B. U. (2016), "Smooth backfitting for additive modeling with small errors-in-variables, with an application to additive functional regression for multiple predictor functions", Bernoulli (accepted).</cite>
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(100)

n &lt;- 100
d &lt;- 2
X &lt;- pnorm(matrix(rnorm(n*d),nrow=n,ncol=d)%*%matrix(c(1,0.6,0.6,1),nrow=2,ncol=2))

f1 &lt;- function(t) 2*(t-0.5)
f2 &lt;- function(t) sin(2*pi*t)

Y &lt;- f1(X[,1])+f2(X[,2])+rnorm(n,0,0.1)

# component function estimation
N &lt;- 101
x &lt;- matrix(rep(seq(0,1,length.out=N),d),nrow=N,ncol=d)
h &lt;- c(0.12,0.08)
  
sbfEst &lt;- SBFitting(Y,x,X,h)
fFit &lt;- sbfEst$SBFit

op &lt;- par(mfrow=c(1,2))
plot(x[,1],f1(x[,1]),type='l',lwd=2,col=2,lty=4,xlab='X1',ylab='Y')
points(x[,1],fFit[,1],type='l',lwd=2,col=1)
points(X[,1],Y,cex=0.3,col=8)
legend('topleft',legend=c('SBF','true'),col=c(1,2),lwd=2,lty=c(1,4),horiz=FALSE,bty='n')
abline(h=0,col=8)

plot(x[,2],f2(x[,2]),type='l',lwd=2,col=2,lty=4,xlab='X2',ylab='Y')
points(x[,2],fFit[,2],type='l',lwd=2,col=1)
points(X[,2],Y,cex=0.3,col=8)
legend('topright',legend=c('SBF','true'),col=c(1,2),lwd=2,lty=c(1,4),horiz=FALSE,bty='n')
abline(h=0,col=8)
par(op)

# prediction
x &lt;- X
h &lt;- c(0.12,0.08)
  
sbfPred &lt;- SBFitting(Y,X,X,h)
fPred &lt;- sbfPred$mY+apply(sbfPred$SBFit,1,'sum')

op &lt;- par(mfrow=c(1,1))
plot(fPred,Y,cex=0.5,xlab='SBFitted values',ylab='Y')
abline(coef=c(0,1),col=8)
par(op)
</code></pre>


</div>