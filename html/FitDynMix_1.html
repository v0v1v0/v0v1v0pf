<div class="container">

<table style="width: 100%;"><tr>
<td>AMLEfit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Estimating a dynamic mixture via AMLE</h2>

<h3>Description</h3>

<p>This function fits a dynamic mixture via Approximate Maximum Likelihood.
Currently only implemented for the lognormal - generalized Pareto case,
with Cauchy or exponential weight.
The bootstrap estimation of the standard errors of the MLEs (used for finding
the supports of the uniform priors) is carried out via parallel computing.
</p>


<h3>Usage</h3>

<pre><code class="language-R">AMLEfit(yObs, epsilon, k, bootreps, intTol = 1e-04, weight)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>yObs</code></td>
<td>
<p>numerical vector: observed sample.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>non-negative scalar: scale parameter of the Markov kernel.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>non-negative integer: number of samples generated in the AMLE
approach, such that k*epsilon = ABC sample size.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bootreps</code></td>
<td>
<p>positive integer: number of bootstrap replications.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intTol</code></td>
<td>
<p>non-negative scalar: threshold for stopping the computation of the integral in the normalization
constant: if the integral on the interval from n-1 to n is smaller than intTol, the approximation procedure stops.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight</code></td>
<td>
<p>'cau' or 'exp': name of weight distribution.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For the lognormal and GPD parameters, the support of the uniform
prior is set equal to the 99% confidence interval of the bootstrap
distribution after discarding the outliers.
For the Cauchy parameters, the support is given by the range of the
bootstrap distribution after discarding the outliers.
Be aware that computing times are large when k and/or bootreps are large.
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<p>AMLEpars a list of four 6 or 5-dimensional vectors: approximate maximum likelihood estimates computed via sample mean,
maxima of the marginal kernel densities, maximum of the multivariate kernel densities,
maximum of the product of the marginal kernel densities.
</p>
<p>ABCsam ((k x epsilon) x nc) matrix: ABC sample, where nc is 6 or 5, according to the weight.
</p>
<p>MLEpars (np x 1) vector: maximum likelihood estimates and
maximized log-likelihood, where np is 7 or 6, according to the weight.
</p>
<p>MLEboot (bootreps x nc) matrix: maximum likelihood estimates obtained in
each bootstrap replication. nc is 6 or 5, according to the weight.
</p>


<h3>References</h3>


<p>Bee M (2023).
“Unsupervised mixture estimation via approximate maximum likelihood based on the Cramér - von Mises distance.”
<em>Computational Statistics &amp; Data Analysis</em>, <b>185</b>, 107764.

</p>


<h3>See Also</h3>

<p><code>AMLEmode</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">k &lt;- 5000
epsilon &lt;- .02
bootreps &lt;- 2
res = AMLEfit(Metro2019, epsilon, k, bootreps, , 'cau')
</code></pre>


</div>