<div class="container">

<table style="width: 100%;"><tr>
<td>classif.DD</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>DD-Classifier Based on DD-plot</h2>

<h3>Description</h3>

<p>Fits Nonparametric Classification Procedure Based on DD–plot
(depth-versus-depth plot) for G dimensions (<code class="reqn">G=g\times h</code>, g
levels and p data depth).
</p>


<h3>Usage</h3>

<pre><code class="language-R">classif.DD(
  group,
  fdataobj,
  depth = "FM",
  classif = "glm",
  w,
  par.classif = list(),
  par.depth = list(),
  control = list(verbose = FALSE, draw = TRUE, col = NULL, alpha = 0.25)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>group</code></td>
<td>
<p>Factor of length <em>n</em> with <em>g</em> levels.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fdataobj</code></td>
<td>
<p><code>data.frame</code>, <code>fdata</code> or <code>list</code>
with the multivariate, functional or both covariates respectively.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>depth</code></td>
<td>
<p>Character vector specifying the type of depth functions to use,
see <code>Details</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>classif</code></td>
<td>
<p>Character vector specifying the type of classifier method to
use, see <code>Details</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>Optional case weights, weights for each value of <code>depth</code>
argument, see <code>Details</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>par.classif</code></td>
<td>
<p>List of parameters for <code>classif</code> procedure.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>par.depth</code></td>
<td>
<p>List of parameters for <code>depth</code> function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>List of parameters for controlling the process.
</p>
<p>If <code>verbose=TRUE</code>, report extra information on progress.
</p>
<p>If <code>draw=TRUE</code> print DD-plot of two samples based on data depth.
</p>
<p><code>col</code>, the colors for points in DD–plot.
</p>
<p><code>alpha</code>, the alpha transparency used in the background of DD–plot, a
number in [0,1].</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Make the group classification of a training dataset using DD-classifier
estimation in the following steps.<br></p>
 
<ol>
<li>
<p> The function computes the selected <code>depth</code> measure of
the points in <code>fdataobj</code> w.r.t. a subsample of each g level group and p
data dimension (<code class="reqn">G=g \times p</code>).  The user can be specify the
parameters for depth function in <code>par.depth</code>.
</p>
<p>(i) Type of depth function from functional data, see <code>Depth</code>:
</p>
 
<ul>
<li> <p><code>"FM"</code>: Fraiman and Muniz depth.  
</p>
</li>
<li> <p><code>"mode"</code>: h–modal depth. 
</p>
</li>
<li> <p><code>"RT"</code>: random Tukey depth. 
</p>
</li>
<li> <p><code>"RP"</code>: random project depth.
</p>
</li>
<li> <p><code>"RPD"</code>: double random project depth.
</p>
</li>
</ul>
<p>(ii) Type of depth function from multivariate functional data, see <code>depth.mfdata</code>:
</p>
 
<ul>
<li> <p><code>"FMp"</code>: Fraiman and Muniz depth with common support.
Suppose that all p–fdata objects have the same support (same rangevals),
see <code>depth.FMp</code>.  
</p>
</li>
<li> <p><code>"modep"</code>: h–modal depth using a p–dimensional metric, see <code>depth.modep</code>.
</p>
</li>
<li> <p><code>"RPp"</code>: random project depth using a p–variate depth with the
projections, see <code>depth.RPp</code>.  
</p>
</li>
</ul>
<p>If the procedure requires to compute a distance such as in <code>"knn"</code> or <code>"np"</code> classifier or
<code>"mode"</code> depth, the user must use a proper distance function:
<code>metric.lp</code> for functional data and <code>metric.dist</code>
for multivariate data.
</p>
<p>(iii) Type of depth function from multivariate data, see
<code>Depth.Multivariate</code>: 
</p>
 
<ul>
<li> <p><code>"SD"</code>: Simplicial depth (for bivariate data).  
</p>
</li>
<li> <p><code>"HS"</code>: Half-space depth.  
</p>
</li>
<li> <p><code>"MhD"</code>: Mahalanobis depth.
</p>
</li>
<li> <p><code>"RD"</code>: random projections depth. 
</p>
</li>
<li> <p><code>"LD"</code>: Likelihood depth.  
</p>
</li>
</ul>
</li>
<li>
<p> The function calculates the misclassification rate based on data depth
computed in step (1) using the following classifiers.
</p>
 
<ul>
<li> <p><code>"MaxD"</code>: Maximum depth.  
</p>
</li>
<li> <p><code>"DD1"</code>: Search the best separating polynomial of degree 1.  
</p>
</li>
<li> <p><code>"DD2"</code>: Search the best separating polynomial of degree 2.
</p>
</li>
<li> <p><code>"DD3"</code>: Search the best separating polynomial of degree 3.
</p>
</li>
<li> <p><code>"glm"</code>: Logistic regression is computed using Generalized Linear Models
<code>classif.glm</code>.  
</p>
</li>
<li> <p><code>"gam"</code>: Logistic regression is computed using Generalized Additive Models
<code>classif.gsam</code>.
</p>
</li>
<li> <p><code>"lda"</code>: Linear Discriminant Analysis is computed using
<code>lda</code>. 
</p>
</li>
<li> <p><code>"qda"</code>: Quadratic Discriminant Analysis is computed using <code>qda</code>.  
</p>
</li>
<li> <p><code>"knn"</code>: k-Nearest Neighbour classification is computed using <code>classif.knn</code>.  
</p>
</li>
<li> <p><code>"np"</code>: Non-parametric Kernel classifier is computed using
<code>classif.np</code>.  
</p>
</li>
</ul>
<p>The user can be specify the parameters for classifier function in <code>par.classif</code> such as the smoothing parameter
<code>par.classif[["h"]]</code>, if <code>classif="np"</code> or the k-Nearest
Neighbour <code>par.classif[["knn"]]</code>, if <code>classif="knn"</code>.
</p>
<p>In the case of polynomial classifier (<code>"DD1"</code>, <code>"DD2"</code> and
<code>"DD3"</code>) uses the original procedure proposed by Li et al. (2012), by
defalut rotating the DD-plot (to exchange abscise and ordinate) using in
<code>par.classif</code> argument <code>rotate=TRUE</code>. Notice that the maximum
depth classifier can be considered as a particular case of DD1, fixing the
slope with a value of 1 (<code>par.classif=list(pol=1)</code>).
</p>
<p>The number of possible different polynomials depends on the sample size
<code>n</code> and increases polynomially with order <code class="reqn">k</code>. In the case of
<code class="reqn">g</code> groups, so the procedure applies some multiple-start optimization
scheme to save time:
</p>

<ul>
<li>
<p> generate all combinations of the elements of n taken k at a time:
<code class="reqn">g \times combn(N,k)</code> candidate solutions, and, when
this number is larger than <code>nmax=10000</code>, a random sample of
<code>10000</code> combinations.
</p>
</li>
<li>
<p> smooth the empirical loss with the logistic function
<code class="reqn">1/(1+e^{-tx})</code>. The classification rule is
constructed optimizing the best <code>noptim</code> combinations in this random
sample (by default <code>noptim=1</code> and <code>tt=50/range(depth values)</code>).
Note that Li et al.  found that the optimization results become stable for
<code class="reqn">t \in [50, 200]</code> when the depth is standardized
with upper bound 1.  </p>
</li>
</ul>
<p> The original procedure (Li et al. (2012)) not need to
try many initial polynomials (<code>nmax=1000</code>) and that the procedure
optimize the best (<code>noptim=1</code>), but we recommended to repeat the last
step for different solutions, as for example <code>nmax=250</code> and
<code>noptim=25</code>. User can change the parameters <code>pol</code>, <code>rotate</code>,
<code>nmax</code>, <code>noptim</code> and <code>tt</code> in the argument <code>par.classif</code>.
</p>
<p>The <code>classif.DD</code> procedure extends to multi-class problems by
incorporating the method of <em>majority voting</em> in the case of polynomial
classifier and the method <em>One vs the Rest</em> in the logistic case
(<code>"glm"</code> and <code>"gam"</code>).
</p>
</li>
</ol>
<h3>Value</h3>


<ul>
<li> <p><code>group.est</code> Estimated vector groups by classified method
selected.  
</p>
</li>
<li> <p><code>misclassification</code>  Probability of misclassification. 
</p>
</li>
<li> <p><code>prob.classification</code>  Probability of correct classification by group level. 
</p>
</li>
<li>  <p><code>dep</code>  Data frame with the depth of the curves for functional data (or points for multivariate data) in
<code>fdataobj</code> w.r.t. each <code>group</code> level. 
</p>
</li>
<li> <p><code>depth</code>  Character vector specifying the type of depth functions used. 
</p>
</li>
<li> <p><code>par.depth</code>  List of parameters for <code>depth</code> function. 
</p>
</li>
<li> <p><code>classif</code>  Type of classifier used. 
</p>
</li>
<li> <p><code>par.classif</code> List of parameters for <code>classif</code> procedure.
</p>
</li>
<li> <p><code>w</code> Optional case weights. 
</p>
</li>
<li> <p><code>fit</code> Fitted object by <code>classif</code> method using the depth as covariate.
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>This version was created by Manuel Oviedo de la Fuente and Manuel
Febrero Bande and includes the original version for polynomial classifier
created by Jun Li, Juan A. Cuesta-Albertos and Regina Y. Liu.
</p>


<h3>References</h3>

<p>Cuesta-Albertos, J.A., Febrero-Bande, M. and Oviedo de la Fuente, M.
<em>The DDG-classifier in the functional setting</em>, (2017). Test, 26(1),
119-142. DOI: <a href="https://doi.org/10.1007/s11749-016-0502-6">doi:10.1007/s11749-016-0502-6</a>.
</p>


<h3>See Also</h3>

<p>See Also as <code>predict.classif.DD</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# DD-classif for functional data
data(tecator)
ab &lt;- tecator$absorp.fdata
ab1 &lt;- fdata.deriv(ab, nderiv = 1)
ab2 &lt;- fdata.deriv(ab, nderiv = 2)
gfat &lt;- factor(as.numeric(tecator$y$Fat&gt;=15))

# DD-classif for p=1 functional  data set
out01 &lt;- classif.DD(gfat,ab,depth="mode",classif="np")
out02 &lt;- classif.DD(gfat,ab2,depth="mode",classif="np")
# DD-plot in gray scale
ctrl&lt; &lt;- list(draw=T,col=gray(c(0,.5)),alpha=.2)
out02bis &lt;- classif.DD(gfat,ab2,depth="mode",classif="np",control=ctrl)

# 2 depth functions (same curves) 
ldat &lt;- mfdata("ab" = ab, "ab2" = ab2)
out03 &lt;- classif.DD(gfat,list(ab2,ab2),depth=c("RP","mode"),classif="np")
# DD-classif for p=2 functional data set
# Weighted version 
out04 &lt;- classif.DD(gfat, ldat, depth="mode",
                    classif="np", w=c(0.5,0.5))
# Model version
out05 &lt;- classif.DD(gfat,ldat,depth="mode",classif="np")
# Integrated version (for multivariate functional data)
out06 &lt;- classif.DD(gfat,ldat,depth="modep",classif="np")

# DD-classif for multivariate data
data(iris)
group &lt;- iris[,5]
x &lt;- iris[,1:4]
out07 &lt;- classif.DD(group,x,depth="LD",classif="lda")
summary(out07)
out08 &lt;- classif.DD(group, list(x,x), depth=c("MhD","LD"),
                    classif="lda")
summary(out08)

# DD-classif for functional data: g levels 
data(phoneme)
mlearn &lt;- phoneme[["learn"]]
glearn &lt;- as.numeric(phoneme[["classlearn"]])-1
out09 &lt;- classif.DD(glearn,mlearn,depth="FM",classif="glm")
out10 &lt;- classif.DD(glearn,list(mlearn,mlearn),depth=c("FM","RP"),classif="glm")
summary(out09)
summary(out10)

## End(Not run)
</code></pre>


</div>