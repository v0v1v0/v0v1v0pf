<div class="container">

<table style="width: 100%;"><tr>
<td>flashClust</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Faster alternative to hclust </h2>

<h3>Description</h3>

<p>This function implements optimal hierarchical clustering with the same interface as
<code>hclust</code>. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">hclust(d, method = "complete", members=NULL)
flashClust(d, method = "complete", members=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>
<p> a dissimilarity structure as produced by 'dist'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p> the agglomeration method to be used. This should be (an
unambiguous abbreviation of) one of <code>"ward"</code>, <code>"single"</code>,
<code>"complete"</code>, <code>"average"</code>, <code>"mcquitty"</code>, <code>"median"</code> or
<code>"centroid"</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>members</code></td>
<td>
<p><code>NULL</code> or a vector with length size of
<code>d</code>. See the ‘Details’ section.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>See the description of <code>hclust</code> for details on available clustering methods.
</p>
<p>If <code>members!=NULL</code>, then <code>d</code> is taken to be a
dissimilarity matrix between clusters instead of dissimilarities
between singletons and <code>members</code> gives the number of observations
per cluster.  This way the hierarchical cluster algorithm can be
‘started in the middle of the dendrogram’, e.g., in order to
reconstruct the part of the tree above a cut (see examples).
Dissimilarities between clusters can be efficiently computed (i.e.,
without <code>hclust</code> itself) only for a limited number of
distance/linkage combinations, the simplest one being squared
Euclidean distance and centroid linkage.  In this case the
dissimilarities between the clusters are the squared Euclidean
distances between cluster means.
</p>
<p><code>flashClust</code> is a wrapper for compatibility with older code.
</p>


<h3>Value</h3>

<p>Returned value is the same as that of <code>hclust</code>: 
An object of class <b>hclust</b> which describes the
tree produced by the clustering process.
The object is a list with components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>merge</code></td>
<td>
<p>an <code class="reqn">n-1</code> by 2 matrix.
Row <code class="reqn">i</code> of <code>merge</code> describes the merging of clusters
at step <code class="reqn">i</code> of the clustering.
If an element <code class="reqn">j</code> in the row is negative,
then observation <code class="reqn">-j</code> was merged at this stage.
If <code class="reqn">j</code> is positive then the merge
was with the cluster formed at the (earlier) stage <code class="reqn">j</code>
of the algorithm.
Thus negative entries in <code>merge</code> indicate agglomerations
of singletons, and positive entries indicate agglomerations
of non-singletons.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>height</code></td>
<td>
<p>a set of <code class="reqn">n-1</code> non-decreasing real values.
The clustering <em>height</em>: that is, the value of
the criterion associated with the clustering
<code>method</code> for the particular agglomeration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>order</code></td>
<td>
<p>a vector giving the permutation of the original
observations suitable for plotting, in the sense that a cluster
plot using this ordering and matrix <code>merge</code> will not have
crossings of the branches.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>labels</code></td>
<td>
<p>labels for each of the objects being clustered.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the call which produced the result.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>the cluster method that has been used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dist.method</code></td>
<td>
<p>the distance that has been used to create <code>d</code>
(only returned if the distance object has a <code>"method"</code>
attribute).</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p> Fionn Murtagh, adapted and packaged by Peter Langfelder</p>


<h3>References</h3>

 
<p>This implementation is mentioned in 
</p>
<p>Peter Langfelder, Steve Horvath (2012)
Fast R Functions for Robust Correlations and Hierarchical Clustering.
Journal of Statistical Software, 46(11), 1-17.
<a href="http://www.jstatsoft.org/v46/i11/">http://www.jstatsoft.org/v46/i11/</a>
</p>
<p>F.Murtagh's software web site: http://www.classification-society.org/csna/mda-sw/ , section 6
</p>
<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
<em>The New S Language</em>.
Wadsworth \&amp; Brooks/Cole. (S version.)
</p>
<p>Everitt, B. (1974).
<em>Cluster Analysis</em>.
London: Heinemann Educ. Books.
</p>
<p>Hartigan, J. A. (1975).
<em>Clustering  Algorithms</em>.
New York: Wiley.
</p>
<p>Sneath, P. H. A. and R. R. Sokal (1973).
<em>Numerical Taxonomy</em>.
San Francisco: Freeman.
</p>
<p>Anderberg, M. R. (1973).
<em>Cluster Analysis for Applications</em>.
Academic Press: New York.
</p>
<p>Gordon, A. D. (1999).
<em>Classification</em>. Second Edition.
London: Chapman and Hall / CRC
</p>
<p>Murtagh, F. (1985).
“Multidimensional Clustering Algorithms”, in
<em>COMPSTAT Lectures 4</em>.
Wuerzburg: Physica-Verlag
(for algorithmic details of algorithms used).
</p>
<p>McQuitty, L.L. (1966).
Similarity Analysis by Reciprocal Pairs for Discrete and Continuous
Data.
<em>Educational and Psychological Measurement</em>, <b>26</b>, 825–831.
</p>


<h3>See Also</h3>

 <p><code>hclust</code> </p>


<h3>Examples</h3>

<pre><code class="language-R">
  # generate some data to cluster
  set.seed(1);
  nNodes = 2000;

  # Random "distance" matrix
  dst = matrix(runif(n = nNodes^2, min = 0, max = 1), nNodes, nNodes);

  # Time the flashClust clustering
  system.time( {
     h1 = hclust(as.dist(dst), method= "average");
    } );

  # Time the standard R clustering
  system.time( {
     h2 = stats::hclust(as.dist(dst), method = "average");
    } );

  all.equal(h1, h2)
  # What is different:

  h1[[6]]
  h2[[6]]

  # Everything but the 'call' component is the same; in particular, the trees are exactly equal.

</code></pre>


</div>