<div class="container">

<table style="width: 100%;"><tr>
<td>kmeansClustering</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
K-Means Clustering
</h2>

<h3>Description</h3>

<p>Perform k-means clustering on a data matrix. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">kmeansClustering(DataOrDistances, ClusterNo,

 Type = 'LBG',RandomNo=5000, CategoricalData,
 
 PlotIt=FALSE, Verbose = FALSE,... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>DataOrDistances</code></td>
<td>
<p>Either nonsymmetric [1:n,1:d] datamatrix of n cases and d numerical features or
</p>
<p>symmetric [1:n,1:n] distance matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Type</code></td>
<td>
 
<p>Choice of Kmeans algorithm, currently either " <code>Hartigan</code>" [Hartigan/Wong, 1979], "<code>LBG</code>" [Linde et al., 1980], "<code>Sparse</code>" sparse k-means proposed in [Witten/Tibshirani, 2010], "<code>Steinley</code>" best method of [Steinley/Brusco, 2007] proposed in Steinley 2003, "<code>Lloyd</code>" [Lloyd, 1982], "<code>Forgy</code>"[Forgy, 1965], <code>MacQueen</code> [MacQueen, 1967], <code>kcentroids</code> [Leisch, 2006], "<code>kprototypes</code>" [Szepannek, 2018], "<code>Pelleg-moore</code>" [Pelleg &amp; Moores,2000], "<code>Elkan</code>" [Elkan, 2003], "<code>kmeans++</code>"" [Arthur &amp; Vassilvitskii], <code>Hamerly</code>"[Hamerly, 2010] ,<code>Dualtree</code>"  or <code>Dualtree-covertree</code> [Curtin, 2017]"
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>RandomNo</code></td>
<td>
<p>Only for " <code>Steinley</code>" or in case of distance matrix, number of random initializations with searching for minimal SSE, see [Steinley/Brusco, 2007]</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CategoricalData</code></td>
<td>
<p>Only for " <code>kprototypes</code>", [1:n,1:m] matrix of categorical features]</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Verbose</code></td>
<td>
<p>Print details, if true</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Further arguments like <code>iter.max</code>, <code>nstart</code>, for <code>kcentroids</code> please see <code>kcca</code> function of the <span class="pkg">flexclust</span> package, or  <code>KMeansSparseCluster</code> 
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Uses either <span class="pkg">stats</span> package function 'kmeans', <span class="pkg">cclust</span> package implemention, <span class="pkg">flexclust</span> package implemention or own code.
In case of a distance matrix, RandomNo should be significantly lower than 5000, otherwise a long computation time is to be expected.
</p>


<h3>Value</h3>

<p>List V of 
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Object</code></td>
<td>
<p> Object of the clustering algorithm used if existent, otherwise
</p>
<p>SumDistsToCentroids: Vector of within-cluster sum of squares, one component per cluster
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Centroids</code></td>
<td>
<p>the final cluster centers.</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>The version using a distance matrix is still in the test phase and not yet verified.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Hartigan/Wong, 1979]  Hartigan, J. A., &amp; Wong, M. A.: Algorithm AS 136: A k-means clustering algorithm, Journal of the Royal Statistical Society. Series C (Applied Statistics), Vol. 28(1), pp. 100-108. 1979.
</p>
<p>[Linde et al., 1980]  Linde, Y., Buzo, A., &amp; Gray, R.: An algorithm for vector quantizer design, IEEE Transactions on communications, Vol. 28(1), pp. 84-95. 1980.
</p>
<p>[Steinley/Brusco, 2007]  Steinley, D., &amp; Brusco, M. J.: Initializing k-means batch clustering: A critical evaluation of several techniques, Journal of Classification, Vol. 24(1), pp. 99-121. 2007.
</p>
<p>[Forgy, 1965]  Forgy, E. W.: Cluster analysis of multivariate data: efficiency versus interpretability of classifications, Biometrics, Vol. 21, pp. 768-769. 1965.
</p>
<p>[MacQueen, 1967]  MacQueen, J.: Some methods for classification and analysis of multivariate observations, Proc. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, Vol. 1, pp. 281-297, Oakland, CA, USA., 1967.
</p>
<p>[Pelleg &amp; Moores,2000] Pelleg, Dan, and Andrew W. Moore. X-means: Extending k-means with efficient estimation of the number of clusters, ICML. Vol. 1. 2000.
</p>
<p>[Elkan, 2003] Elkan, Charles: Using the triangle inequality to acceler- ate k-means, In Tom Fawcett and Nina Mishra, editors, ICML, pages Vol.3, 147-153. AAAI Press, 2003.
</p>
<p>[Lloyd, 1982]  Lloyd, S.: Least squares quantization in PCM, IEEE transactions on information theory, Vol. 28(2), pp. 129-137. 1982.
</p>
<p>[Leisch, 2006]  Leisch, F.: A toolbox for k-centroids cluster analysis, Computational Statistics &amp; Data Analysis, Vol. 51(2), pp. 526-544. 2006.
</p>
<p>[Arthur &amp; Vassilvitskii] Arthur, David, and  Vassilvitskii, Sergei: K-means++ the advantages of careful seeding, Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. 2007
</p>
<p>[Witten/Tibshirani, 2010] Witten, D. and Tibshirani, R.: A Framework for
Feature Selection in Clustering. Journal of the American Statistical
Association, Vol. 105(490), pp. 713-726, 2010.
</p>
<p>[Hamerly, 2010]  Hamerly, Greg: Making k-means even faster, Proceedings of the 2010 SIAM international conference on data mining, Society for Industrial and Applied Mathematics, pp. 130-140, 2010.
</p>
<p>[Szepannek, 2018] Szepannek, G.: clustMixType: User-Friendly Clustering of Mixed-Type Data in R, The R Journal, Vol. 10/2, pp. 200-208, doi:10.32614/RJ2018048, 2018.
</p>
<p>[Curtin, 2017]  Curtin, Ryan R: A dual-tree algorithm for fast k-means clustering with large k, Proceedings of the 2017 SIAM International Conference on Data Mining, Society for Industrial and Applied Mathematics, 2017.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data('Hepta')
out=kmeansClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)


data('Leukemia')
# As expected does not perform well
# For non-spherical cluster structures:
out=kmeansClustering(Leukemia$DistanceMatrix,ClusterNo=6,RandomNo =10,PlotIt=TRUE)




data('Hepta')
out=kmeansClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE,Type="Steinley")



data('Hepta')
out=kmeansClustering(Hepta$Data,ClusterNo = 7,
Type = "kprototypes",CategoricalData = as.matrix(Hepta$Cls))


</code></pre>


</div>