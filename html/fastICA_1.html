<div class="container">

<table style="width: 100%;"><tr>
<td>fastICA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>FastICA algorithm</h2>

<h3>Description</h3>

<p>This is an R and C code implementation of the FastICA algorithm
of Aapo Hyvarinen et al. (<a href="https://www.cs.helsinki.fi/u/ahyvarin/">https://www.cs.helsinki.fi/u/ahyvarin/</a>) to
perform Independent Component Analysis (ICA) and Projection Pursuit.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fastICA(X, n.comp, alg.typ = c("parallel","deflation"),
        fun = c("logcosh","exp"), alpha = 1.0, method = c("R","C"),
        row.norm = FALSE, maxit = 200, tol = 1e-04, verbose = FALSE,
        w.init = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>a data matrix with <code>n</code> rows representing observations
and <code>p</code> columns representing variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.comp</code></td>
<td>
<p>number of components to be extracted</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alg.typ</code></td>
<td>
<p>if <code>alg.typ == "parallel"</code> the components are extracted
simultaneously (the default). if <code>alg.typ == "deflation"</code> the
components are extracted one at a time.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fun</code></td>
<td>
<p>the functional form of the <code class="reqn">G</code> function used in the
approximation to neg-entropy (see ‘details’).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>constant in range [1, 2] used in approximation to
neg-entropy when <code>fun == "logcosh"</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>if <code>method == "R"</code> then computations are done
exclusively in <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> (default). The code allows the interested <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> user to
see exactly what the algorithm does.
if <code>method == "C"</code> then C code is used to perform most of the
computations, which makes the algorithm run faster.  During
compilation the C code is linked to an optimized BLAS library if
present, otherwise stand-alone BLAS routines are compiled.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>row.norm</code></td>
<td>
<p>a logical value indicating whether rows of the data
matrix <code>X</code> should be standardized beforehand.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>maximum number of iterations to perform.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>a positive scalar giving the tolerance at which the
un-mixing matrix is considered to have converged.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>a logical value indicating the level of output as the
algorithm runs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w.init</code></td>
<td>
<p>Initial un-mixing matrix of dimension
<code>c(n.comp, n.comp)</code>. If <code>NULL</code> (default) then a matrix of
normal r.v.'s is used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><b>Independent Component Analysis (ICA)</b>
</p>
<p>The data matrix X is considered to be a linear combination of
non-Gaussian (independent) components i.e. X = SA where columns of S
contain the independent components and A is a linear mixing matrix. In
short ICA attempts to ‘un-mix’ the data by estimating an
un-mixing matrix W where XW = S.
</p>
<p>Under this generative model the measured ‘signals’ in X will
tend to be ‘more Gaussian’ than the source components (in S) due to
the Central Limit Theorem. Thus, in order to extract the independent
components/sources we search for an un-mixing matrix W that maximizes
the non-gaussianity of the sources.
</p>
<p>In FastICA, non-gaussianity is measured using approximations to
neg-entropy (<code class="reqn">J</code>) which are more robust than kurtosis-based
measures and fast to compute.
</p>
<p>The approximation takes the form
</p>
<p style="text-align: center;"><code class="reqn">J(y) = [E\{G(y)\}-E\{G(v)\}]^2</code>
</p>

<p>where <code class="reqn">v</code> is a N(0,1) r.v.
</p>
<p>The following choices of G are included as options
<code class="reqn">G(u)=\frac{1}{\alpha} \log \cosh (\alpha u)</code>
and <code class="reqn">G(u)=-\exp(u^2/2)</code>.
</p>
<p><b>Algorithm</b>
</p>
<p>First, the data are centered by subtracting the mean of each column of the
data matrix X.
</p>
<p>The data matrix is then ‘whitened’ by projecting the data onto
its principal component directions i.e. X -&gt; XK where K is a
pre-whitening matrix. The number of components can be specified by the
user.
</p>
<p>The ICA algorithm then estimates a matrix W s.t XKW = S . W is chosen to
maximize the neg-entropy approximation under the constraints that W is
an orthonormal matrix. This constraint ensures that the estimated
components are uncorrelated. The algorithm is based on a fixed-point
iteration scheme for maximizing the neg-entropy.
</p>
<p><b>Projection Pursuit</b>
</p>
<p>In the absence of a generative model for the data the algorithm can be
used to find the projection pursuit directions. Projection pursuit is
a technique for finding ‘interesting’ directions in multi-dimensional
datasets. These projections and are useful for visualizing the dataset
and in density estimation and regression. Interesting directions are
those which show the least Gaussian distribution, which is what the
FastICA algorithm does.
</p>


<h3>Value</h3>

<p>A list containing the following components
</p>
<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>pre-processed data matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>pre-whitening matrix that projects data onto the first <code>n.comp</code>
principal components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>W</code></td>
<td>
<p>estimated un-mixing matrix (see definition in details)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>A</code></td>
<td>
<p>estimated mixing matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>S</code></td>
<td>
<p>estimated source matrix</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>J L Marchini and C Heaton 
</p>


<h3>References</h3>

<p>A. Hyvarinen and E. Oja (2000)
Independent Component Analysis: Algorithms and Applications,
<em>Neural Networks</em>, <b>13(4-5)</b>:411-430
</p>


<h3>See Also</h3>

<p><code>ica.R.def</code>, <code>ica.R.par</code></p>


<h3>Examples</h3>

<pre><code class="language-R">#---------------------------------------------------
#Example 1: un-mixing two mixed independent uniforms
#---------------------------------------------------

S &lt;- matrix(runif(10000), 5000, 2)
A &lt;- matrix(c(1, 1, -1, 3), 2, 2, byrow = TRUE)
X &lt;- S %*% A

a &lt;- fastICA(X, 2, alg.typ = "parallel", fun = "logcosh", alpha = 1, 
             method = "C", row.norm = FALSE, maxit = 200, 
             tol = 0.0001, verbose = TRUE)

par(mfrow = c(1, 3))
plot(a$X, main = "Pre-processed data")
plot(a$X %*% a$K, main = "PCA components")
plot(a$S, main = "ICA components")

#--------------------------------------------
#Example 2: un-mixing two independent signals
#--------------------------------------------

S &lt;- cbind(sin((1:1000)/20), rep((((1:200)-100)/100), 5))
A &lt;- matrix(c(0.291, 0.6557, -0.5439, 0.5572), 2, 2)
X &lt;- S %*% A

a &lt;- fastICA(X, 2, alg.typ = "parallel", fun = "logcosh", alpha = 1, 
             method = "R", row.norm = FALSE, maxit = 200, 
             tol = 0.0001, verbose = TRUE)

par(mfcol = c(2, 3))
plot(1:1000, S[,1 ], type = "l", main = "Original Signals", 
     xlab = "", ylab = "")
plot(1:1000, S[,2 ], type = "l", xlab = "", ylab = "")
plot(1:1000, X[,1 ], type = "l", main = "Mixed Signals", 
     xlab = "", ylab = "")
plot(1:1000, X[,2 ], type = "l", xlab = "", ylab = "")
plot(1:1000, a$S[,1 ], type = "l", main = "ICA source estimates", 
     xlab = "", ylab = "")
plot(1:1000, a$S[, 2], type = "l", xlab = "", ylab = "")

#-----------------------------------------------------------
#Example 3: using FastICA to perform projection pursuit on a
#           mixture of bivariate normal distributions
#-----------------------------------------------------------

if(require(MASS)){
x &lt;- mvrnorm(n = 1000, mu = c(0, 0), Sigma = matrix(c(10, 3, 3, 1), 2, 2))
x1 &lt;- mvrnorm(n = 1000, mu = c(-1, 2), Sigma = matrix(c(10, 3, 3, 1), 2, 2))
X &lt;- rbind(x, x1)

a &lt;- fastICA(X, 2, alg.typ = "deflation", fun = "logcosh", alpha = 1,
             method = "R", row.norm = FALSE, maxit = 200, 
             tol = 0.0001, verbose = TRUE)

par(mfrow = c(1, 3))
plot(a$X, main = "Pre-processed data")
plot(a$X %*% a$K, main = "PCA components")
plot(a$S, main = "ICA components")
}
</code></pre>


</div>