<div class="container">

<table style="width: 100%;"><tr>
<td>KLdiv</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kullback-Leibler Divergence</h2>

<h3>Description</h3>

<p>Estimate the Kullback-Leibler divergence of several distributions.</p>


<h3>Usage</h3>

<pre><code class="language-R">## S4 method for signature 'matrix'
KLdiv(object, eps = 10^-4, overlap = TRUE,...)
## S4 method for signature 'flexmix'
KLdiv(object, method = c("continuous", "discrete"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>See Methods section below.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>The method to be used; "continuous" determines
the Kullback-Leibler divergence between the unweighted theoretical
component distributions and the unweighted posterior probabilities
at the observed points are used by "discrete".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Probabilities below this threshold are replaced by this
threshold for numerical stability.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>overlap</code></td>
<td>
<p>Logical, do not determine the KL divergence for
those pairs where for each point at least one of the densities has a
value smaller than <code>eps</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Passed to the matrix method.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Estimates </p>
<p style="text-align: center;"><code class="reqn">\int f(x) (\log f(x) - \log g(x)) dx</code>
</p>

<p>for distributions with densities <code class="reqn">f()</code> and <code class="reqn">g()</code>.
</p>


<h3>Value</h3>

<p>A matrix of KL divergences where the rows correspond to using the
respective distribution as <code class="reqn">f()</code> in the formula above.
</p>


<h3>Methods</h3>


<dl>
<dt>object = "matrix":</dt>
<dd>
<p>Takes as input a matrix of
density values with one row per observation and one column per
distribution.</p>
</dd>
<dt>object = "flexmix":</dt>
<dd>
<p>Returns the Kullback-Leibler divergence
of the mixture components.</p>
</dd>
</dl>
<h3>Note</h3>

<p>The density functions are modified to have equal support.
A weight of at least <code>eps</code> is given to each
observation point for the modified densities.
</p>


<h3>Author(s)</h3>

<p>Friedrich Leisch and Bettina Gruen</p>


<h3>References</h3>

<p>S. Kullback and R. A. Leibler. On information and sufficiency.<em>The
Annals of Mathematical Statistics</em>, <b>22</b>(1), 79–86, 1951.
</p>
<p>Friedrich Leisch. Exploring the structure of mixture model
components. In Jaromir Antoch, editor, Compstat 2004–Proceedings in
Computational Statistics, 1405–1412. Physika Verlag, Heidelberg,
Germany, 2004. ISBN 3-7908-1554-3.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Gaussian and Student t are much closer to each other than
## to the uniform:

x &lt;- seq(-3, 3, length = 200)
y &lt;- cbind(u = dunif(x), n = dnorm(x), t = dt(x, df = 10))

matplot(x, y, type = "l")
KLdiv(y)

if (require("mlbench")) {
set.seed(2606)
x &lt;-  mlbench.smiley()$x
model1 &lt;- flexmix(x ~ 1, k = 9, model = FLXmclust(diag = FALSE),
                  control  =  list(minprior = 0))
plotEll(model1, x)
KLdiv(model1)
}
</code></pre>


</div>