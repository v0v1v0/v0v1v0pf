<div class="container">

<table style="width: 100%;"><tr>
<td>continuous_entropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Shannon entropy for a continuous pdf</h2>

<h3>Description</h3>

<p>Computes the Shannon entropy <code class="reqn">\mathcal{H}(p)</code> for a continuous 
probability density function (pdf) <code class="reqn">p(x)</code> using numerical integration.
</p>


<h3>Usage</h3>

<pre><code class="language-R">continuous_entropy(pdf, lower, upper, base = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>pdf</code></td>
<td>
<p>R function for the pdf <code class="reqn">p(x)</code> of a RV <code class="reqn">X \sim p(x)</code>. This function must
be non-negative and integrate to <code class="reqn">1</code> over the interval [<code>lower</code>, <code>upper</code>].</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower, upper</code></td>
<td>
<p>lower and upper integration limit. <code>pdf</code> must integrate to 
<code>1</code> on this interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>base</code></td>
<td>
<p>logarithm base; entropy is measured in “nats” for 
<code>base = exp(1)</code>; in “bits” if <code>base = 2</code> (default).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Shannon entropy of a continuous random variable (RV) <code class="reqn">X \sim p(x)</code> is defined as
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{H}(p) = -\int_{-\infty}^{\infty} p(x) \log p(x) d x.
</code>
</p>

<p>Contrary to discrete RVs, continuous RVs can have negative entropy (see Examples).
</p>


<h3>Value</h3>

<p>scalar; entropy value (real). 
</p>
<p>Since <code>continuous_entropy</code> uses numerical integration (<code>integrate()</code>) convergence
is not garantueed (even if integral in definition of <code class="reqn">\mathcal{H}(p)</code> exists).
Issues a warning if <code>integrate()</code> does not converge.
</p>


<h3>See Also</h3>

<p><code>discrete_entropy</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># entropy of U(a, b) = log(b - a). Thus not necessarily positive anymore, e.g.
continuous_entropy(function(x) dunif(x, 0, 0.5), 0, 0.5) # log2(0.5)

# Same, but for U(-1, 1)
my_density &lt;- function(x){
  dunif(x, -1, 1)
}
continuous_entropy(my_density, -1, 1) # = log(upper - lower)

# a 'triangle' distribution
continuous_entropy(function(x) x, 0, sqrt(2))

</code></pre>


</div>