<div class="container">

<table style="width: 100%;"><tr>
<td>anisotropic_Kronecker</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kronecker product or row tensor product of two base-learners with anisotropic penalty</h2>

<h3>Description</h3>

<p>Kronecker product or row tensor product of two base-learners allowing for anisotropic penalties. 
For the Kronecker product, <code>%A%</code> works in the general case, <code>%A0%</code> for the special case where 
the penalty is zero in one direction. 
For the row tensor product, <code>%Xa0%</code> works for the special case where 
the penalty is zero in one direction.
</p>


<h3>Usage</h3>

<pre><code class="language-R">bl1 %A% bl2

bl1 %A0% bl2

bl1 %Xa0% bl2
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>bl1</code></td>
<td>
<p>base-learner 1, e.g. <code>bbs(x1)</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bl2</code></td>
<td>
<p>base-learner 2, e.g. <code>bbs(x2)</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>When <code>%O%</code> is called with a specification of <code>df</code> in both base-learners, 
e.g. <code>bbs(x1, df = df1) %O% bbs(t, df = df2)</code>, the global <code>df</code> for the 
Kroneckered base-learner is computed as <code>df = df1 * df2</code>. 
And thus the penalty has only one smoothness parameter lambda resulting in an isotropic penalty, 
</p>
<p style="text-align: center;"><code class="reqn">P = lambda * [(P1 o I) + (I o P2)],</code>
</p>
 
<p>with overall penalty <code class="reqn">P</code>, Kronecker product <code class="reqn">o</code>, 
marginal penalty matrices <code class="reqn">P1, P2</code> and identity matrices <code class="reqn">I</code>.  
(Currie et al. (2006) introduced the generalized linear array model, which has a design matrix that 
is composed of the Kronecker product of two marginal design matrices, which was implemented in mboost 
as <code>%O%</code>.  
See Brockhaus et al. (2015) for the application of array models to functional data.)  
</p>
<p>In contrast, a Kronecker product with anisotropic penalty is obtained by <code>%A%</code>, 
which allows for a different amount of smoothness in the two directions. 
For example <code>bbs(x1, df = df1) %A% bbs(t, df = df2)</code> results in computing two
different values for lambda for the two marginal design matrices and a global value of 
lambda to adjust for the global <code>df</code>, i.e. 
</p>
<p style="text-align: center;"><code class="reqn">P = lambda * [(lambda1 * P1 o I) +  (I o lambda2 * P2)],</code>
</p>
 
<p>with Kronecker product <code class="reqn">o</code>, 
where <code class="reqn">lambda1</code> is computed individually for <code class="reqn">df1</code> and <code class="reqn">P1</code>, 
<code class="reqn">lambda2</code> is computed individually for <code class="reqn">df2</code>  and <code class="reqn">P2</code>, 
and <code class="reqn">lambda</code> is computed such that the global <code class="reqn">df</code> hold <code class="reqn">df = df1 * df2</code>. 
For the computation of <code class="reqn">lambda1</code> and <code class="reqn">lambda2</code> weights specified in the model 
call can only be used when the weights, are such that they are specified on the level 
of rows and columns of the response matrix Y, e.g. resampling weights on the level of 
rows of Y and integration weights on the columns of Y are possible. 
If this the weights cannot be separated to blg1 and blg2 all
weights are set to 1 for the computation of <code class="reqn">lambda1</code> and <code class="reqn">lambda2</code> which implies that 
<code class="reqn">lambda1</code> and <code class="reqn">lambda2</code> are equal over 
folds of <code>cvrisk</code>. The computation of the global <code class="reqn">lambda</code> considers the 
specified <code>weights</code>, such the global <code class="reqn">df</code> are correct.    
</p>
<p>The operator <code>%A0%</code> treats the important special case where <code class="reqn">lambda1 = 0</code> or 
<code class="reqn">lambda2 = 0</code>. In this case it suffices to compute the global lambda and computation gets
faster and arbitrary weights can be specified. Consider <code class="reqn">lambda1 = 0</code> then the penalty becomes 
</p>
<p style="text-align: center;"><code class="reqn">P = lambda * [(1 * P1 o I) +  (I o lambda2 * P2)] = lambda * lambda2 * (I o P2),</code>
</p>
  
<p>and only one global <code class="reqn">lambda</code> is computed which is then  <code class="reqn">lambda * lambda2</code>.  
</p>
<p>If the <code>formula</code> in <code>FDboost</code> contains base-learners connected by 
<code>%O%</code>, <code>%A%</code> or <code>%A0%</code>, 
those effects are not expanded with <code>timeformula</code>, allowing for model specifications 
with different effects in time-direction.  
</p>
<p><code>%Xa0%</code> computes like <code>%X%</code> the row tensor product of two base-learners, 
with the difference that it sets the penalty for one direction to zero. 
Thus, <code>%Xa0%</code> behaves to <code>%X%</code> analogously like <code>%A0%</code> to <code>%O%</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>blg</code> (base-learner generator) with a <code>dpp</code> function 
as for other <code>baselearners</code>.
</p>


<h3>References</h3>

<p>Brockhaus, S., Scheipl, F., Hothorn, T. and Greven, S. (2015): 
The functional linear array model. Statistical Modelling, 15(3), 279-300. 
</p>
<p>Currie, I.D., Durban, M. and Eilers P.H.C. (2006):  
Generalized linear array models with applications to multidimensional smoothing. 
Journal of the Royal Statistical Society, Series B-Statistical Methodology, 68(2), 259-280.
</p>


<h3>Examples</h3>

<pre><code class="language-R"> 
######## Example for anisotropic penalty  
data("viscosity", package = "FDboost") 
## set time-interval that should be modeled
interval &lt;- "101"

## model time until "interval" and take log() of viscosity
end &lt;- which(viscosity$timeAll == as.numeric(interval))
viscosity$vis &lt;- log(viscosity$visAll[,1:end])
viscosity$time &lt;- viscosity$timeAll[1:end]
# with(viscosity, funplot(time, vis, pch = 16, cex = 0.2))

## isotropic penalty, as timeformula is kroneckered to each effect using %O% 
## only for the smooth intercept %A0% is used, as 1-direction should not be penalized 
mod1 &lt;- FDboost(vis ~ 1 + 
                bolsc(T_C, df = 1) + 
                bolsc(T_A, df = 1) + 
                bols(T_C, df = 1) %Xc% bols(T_A, df = 1),
                timeformula = ~ bbs(time, df = 3),
                numInt = "equal", family = QuantReg(),
                offset = NULL, offset_control = o_control(k_min = 9),
                data = viscosity, control=boost_control(mstop = 100, nu = 0.4))
## cf. the formula that is passed to mboost
mod1$formulaMboost

## anisotropic effects using %A0%, as lambda1 = 0 for all base-learners
## in this case using %A% gives the same model, but three lambdas are computed explicitly 
mod1a &lt;- FDboost(vis ~ 1 + 
                bolsc(T_C, df = 1) %A0% bbs(time, df = 3) + 
                bolsc(T_A, df = 1) %A0% bbs(time, df = 3) + 
                bols(T_C, df = 1) %Xc% bols(T_A, df = 1) %A0% bbs(time, df = 3),
                timeformula = ~ bbs(time, df = 3),
                numInt = "equal", family = QuantReg(),
                offset = NULL, offset_control = o_control(k_min = 9),
                data = viscosity, control=boost_control(mstop = 100, nu = 0.4)) 
## cf. the formula that is passed to mboost
mod1a$formulaMboost

## alternative model specification by using a 0-matrix as penalty 
## only works for bolsc() as in bols() one cannot specify K 
## -&gt; model without interaction term 
K0 &lt;- matrix(0, ncol = 2, nrow = 2)
mod1k0 &lt;- FDboost(vis ~ 1 + 
                 bolsc(T_C, df = 1, K = K0) +
                 bolsc(T_A, df = 1, K = K0), 
                 timeformula = ~ bbs(time, df = 3), 
                 numInt = "equal", family = QuantReg(), 
                 offset = NULL, offset_control = o_control(k_min = 9), 
                 data = viscosity, control=boost_control(mstop = 100, nu = 0.4))
## cf. the formula that is passed to mboost
mod1k0$formulaMboost
                
## optimize mstop for mod1, mod1a and mod1k0
## ...
                
## compare estimated coefficients

oldpar &lt;- par(mfrow=c(4, 2))
plot(mod1, which = 1)
plot(mod1a, which = 1)
plot(mod1, which = 2)
plot(mod1a, which = 2)
plot(mod1, which = 3)
plot(mod1a, which = 3)
funplot(mod1$yind, predict(mod1, which=4))
funplot(mod1$yind, predict(mod1a, which=4))
par(oldpar)


</code></pre>


</div>