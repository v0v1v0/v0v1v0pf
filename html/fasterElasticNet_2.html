<div class="container">

<table style="width: 100%;"><tr>
<td>elasticnet</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
A fast way fitting elastic net using RcppArmadillo
</h2>

<h3>Description</h3>

<p>Elastic net is a regularization and variable selection method which linearly combines the L1 penalty of the lasso and L2 penalty of ridge methods. Based on this method, elastic- net is designed to return the trace of finding the best linear regression model. Compared with the existed R version of ElasticNet, our version speeds up the algorithm by using Cholesky decomposition, Givens rotation and RcppArmadillo.
</p>


<h3>Usage</h3>

<pre><code class="language-R">elasticnet(XTX, XTY, lam2, lam1 = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>XTX</code></td>
<td>

<p>The product of the transpose of independent variable X and itself.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>XTY</code></td>
<td>

<p>The product of the transpose of independent variable X and response variable Y
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam1</code></td>
<td>

<p>Penalty of L1-norm. No L1 penalty when lam1 = -1
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam2</code></td>
<td>

<p>Penalty of L2-norm, a  hyper-paramater
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>When only lambda2 is given, elasticnet will return the trace of variable selection with lambda1 decreasing from lambda1_0 to zero. lambda1_0 is a value for lambda1 when there is only one predictor (the one most correlated with the response variable) in the model.
</p>
<p>If lambda1 and lambda2 are both given, it will also return a trace. But in this case, the trace will stop when lambda1 and lambda2 reach the given ones.
</p>
<p>To speed up the algorithm, we use some calculational tricks:
</p>
<p>In the consideration of the low efficiency of R dealing with high-dimensional matrix, we use lower triangular matrices during the iteration of the algorithm to avoid massive matrix calculations. When adding one predictor into the model, we update XTX by recalcuting the lower triangular matrix in the Cholesky decomposition of it. While re- moving one predictor from the model, we update the lower triangular matrix with the help of Givens rotations.
</p>
<p>Furthermore, due to the low efficiency of R dealing with loops, we rewrite the entire algorithm with RcppArmadillo, a C++ linear algebra library.
</p>


<h3>Value</h3>

<p>A list will be returned. When only lambda2 is given, the returned list contains the trace of lambda1 (relamb) and the corresponding coefficients of the predictors (reb). If both lambda1 and lambda2 are given, the corresponding coefficients of the predictors will be returned.
</p>


<h3>Examples</h3>

<pre><code class="language-R">    #Use R built-in datasets mtcars for a model fitting
    x &lt;- as.matrix(mtcars[,-1])
    y &lt;- as.matrix(mtcars[, 1])

    XTX &lt;- t(x) %*% x
    XTY &lt;- t(x) %*% y

    #Prints the output of elastic net model with lambda2 = 0
    res &lt;- elasticnet(XTX,XTY,lam2 = 0)
</code></pre>


</div>