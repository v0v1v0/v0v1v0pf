<div class="container">

<table style="width: 100%;"><tr>
<td>funFEM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
The funFEM algorithm for the clustering of functional data.
</h2>

<h3>Description</h3>

<p>The funFEM algorithm allows to cluster time series or, more generally, functional data. It is based on a discriminative functional mixture model which allows the clustering of the data in a unique and discriminative functional subspace. This model presents the advantage to be parsimonious and can therefore handle long time series.
</p>


<h3>Usage</h3>

<pre><code class="language-R">funFEM(fd, K=2:6, model = "AkjBk", crit = "bic", init = "kmeans", Tinit = c(), maxit = 50,
  eps = 1e-06, disp = FALSE, lambda = 0, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>fd</code></td>
<td>

<p>a functional data object produced by the fda package.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>

<p>an integer vector specifying the numbers of mixture components (clusters) among which the model selection criterion will choose the most appropriate number of groups. Default is 2:6.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>

<p>a vector of discriminative latent mixture (DLM) models to fit. There are 12 different models: "DkBk", "DkB", "DBk", "DB", "AkjBk", "AkjB", "AkBk", "AkBk", "AjBk", "AjB", "ABk", "AB".  The option "all" executes the funFEM algorithm on the 12 models and select the best model according to the maximum value obtained by model selection criterion.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>crit</code></td>
<td>

<p>the criterion to be used for model selection ('bic', 'aic' or 'icl'). 'bic' is the default.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init</code></td>
<td>

<p>the initialization type ('random', 'kmeans' of 'hclust'). 'kmeans' is the default.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Tinit</code></td>
<td>

<p>a n x K matrix which contains posterior probabilities for initializing the algorithm (each line corresponds to an individual).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>

<p>the maximum number of iterations before the stop of the Fisher-EM algorithm. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>

<p>the threshold value for the likelihood differences to stop the Fisher-EM algorithm.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>disp</code></td>
<td>

<p>if true, some messages are printed during the clustering. Default is false.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>

<p>the l0 penalty (between 0 and 1) for the sparse version. See (Bouveyron et al., 2014) for details. Default is 0.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>graph</code></td>
<td>

<p>if true, it plots the evolution of the log-likelhood. Default is false.
</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list is returned: 
</p>
<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>the model name.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>the number of groups.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cls</code></td>
<td>
<p>the group membership of each individual estimated by the Fisher-EM algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>P</code></td>
<td>
<p>the posterior probabilities of each individual for each group.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prms</code></td>
<td>
<p>the model parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>U</code></td>
<td>
<p>the orientation of the functional subspace according to the basis functions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>aic</code></td>
<td>
<p>the value of the Akaike information criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bic</code></td>
<td>
<p>the value of the Bayesian information criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>icl</code></td>
<td>
<p>the value of the integrated completed likelihood criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loglik</code></td>
<td>
<p>the log-likelihood values computed at each iteration of the FEM algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ll</code></td>
<td>
<p>the log-likelihood value obtained at the last iteration of the FEM algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nbprm</code></td>
<td>
<p>the number of free parameters in the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the call of the function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot</code></td>
<td>
<p>some information to pass to the plot.fem function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>crit</code></td>
<td>
<p>the model selction criterion used.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Charles Bouveyron
</p>


<h3>References</h3>

<p>C. Bouveyron, E. CÃ´me and J. Jacques, The discriminative functional mixture model for the analysis of bike sharing systems, Preprint HAL n.01024186, University Paris Descartes, 2014.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Clustering the well-known "Canadian temperature" data (Ramsay &amp; Silverman)
basis &lt;- create.bspline.basis(c(0, 365), nbasis=21, norder=4)
fdobj &lt;- smooth.basis(day.5, CanadianWeather$dailyAv[,,"Temperature.C"],basis,
        fdnames=list("Day", "Station", "Deg C"))$fd
res = funFEM(fdobj,K=4)

# Visualization of the partition and the group means
par(mfrow=c(1,2))
plot(fdobj); lines(fdobj,col=res$cls,lwd=2,lty=1)
fdmeans = fdobj; fdmeans$coefs = t(res$prms$my)
plot(fdmeans); lines(fdmeans,col=1:max(res$cls),lwd=2)

# Visualization in the discriminative subspace (projected scores)
par(mfrow=c(1,1))
plot(t(fdobj$coefs) %*% res$U,col=res$cls,pch=19,main="Discriminative space")


###############################################################################
# Analysis of the Velib data set

# Load the velib data and smoothing
data(velib)
basis&lt;- create.fourier.basis(c(0, 181), nbasis=25)
fdobj &lt;- smooth.basis(1:181,t(velib$data),basis)$fd

# Clustrering with FunFEM
res = funFEM(fdobj,K=6,model='AkjBk',init='kmeans',lambda=0,disp=TRUE)

# Visualization of group means
fdmeans = fdobj; fdmeans$coefs = t(res$prms$my)
plot(fdmeans); lines(fdmeans,col=1:res$K,lwd=2,lty=1)
axis(1,at=seq(5,181,6),labels=velib$dates[seq(5,181,6)],las=2)

# # Choice of K (may be long!)
# res = funFEM(fdobj,K=2:20,model='AkjBk',init='kmeans',lambda=0,disp=TRUE)
# plot(2:20,res$plot$bic,type='b',xlab='K',main='BIC')

# Computation of the closest stations from the group means
par(mfrow=c(3,2))
for (i in 1:res$K) {
  matplot(t(velib$data[which.max(res$P[,i]),]),type='l',lty=i,col=i,xaxt='n',
          lwd=2,ylim=c(0,1))
  axis(1,at=seq(5,181,6),labels=velib$dates[seq(5,181,6)],las=2)
  title(main=paste('Cluster',i,' - ',velib$names[which.max(res$P[,i])]))
}

# Visualization in the discriminative subspace (projected scores)
par(mfrow=c(1,1))
plot(t(fdobj$coefs) %*% res$U,col=res$cls,pch=19,main="Discriminative space")
text(t(fdobj$coefs) %*% res$U)

# # Spatial visualization of the clustering (with library ggmap)
# library(ggmap)
# Mymap = get_map(location = 'Paris', zoom = 12, maptype = 'terrain')
# ggmap(Mymap) + geom_point(data=velib$position,aes(longitude,latitude),
#                           colour = I(res$cl), size = I(3))

# FunFEM clustering with sparsity
res2 = funFEM(fdobj,K=res$K,model='AkjBk',init='user',Tinit=res$P,
              lambda=0.01,disp=TRUE)

# Visualization of group means and the selected functional bases
split.screen(c(2,1))
fdmeans = fdobj; fdmeans$coefs = t(res2$prms$my)
screen(1); plot(fdmeans,col=1:res2$K,xaxt='n',lwd=2) 
axis(1,at=seq(5,181,6),labels=velib$dates[seq(5,181,6)],las=2)
basis$dropind = which(rowSums(abs(res2$U))==0)
screen(2); plot(basis,col=1,lty=1,xaxt='n',xlab='Disc. basis functions')
axis(1,at=seq(5,181,6),labels=velib$dates[seq(5,181,6)],las=2)
close.screen(all=TRUE)

</code></pre>


</div>