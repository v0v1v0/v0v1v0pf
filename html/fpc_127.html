<div class="container">

<table style="width: 100%;"><tr>
<td>kmeansCBI</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Interface functions for clustering methods</h2>

<h3>Description</h3>

<p>These functions provide an interface to several clustering methods
implemented in R, for use together with the cluster stability
assessment in <code>clusterboot</code> (as parameter
<code>clustermethod</code>; "CBI" stands for "clusterboot interface").
In some situations it could make sense to use them to compute a
clustering even if you don't want to run <code>clusterboot</code>, because
some of the functions contain some additional features (e.g., normal
mixture model based clustering of dissimilarity matrices projected
into the Euclidean space by MDS or partitioning around medoids with
estimated number of clusters, noise/outlier identification in
hierarchical clustering).   
</p>


<h3>Usage</h3>

<pre><code class="language-R">kmeansCBI(data,krange,k,scaling=FALSE,runs=1,criterion="ch",...)

hclustCBI(data,k,cut="number",method,scaling=TRUE,noisecut=0,...)

hclusttreeCBI(data,minlevel=2,method,scaling=TRUE,...)

disthclustCBI(dmatrix,k,cut="number",method,noisecut=0,...)



noisemclustCBI(data,G,k,modelNames,nnk,hcmodel=NULL,Vinv=NULL,
                        summary.out=FALSE,...)

distnoisemclustCBI(dmatrix,G,k,modelNames,nnk,
                        hcmodel=NULL,Vinv=NULL,mdsmethod="classical",
                        mdsdim=4, summary.out=FALSE, points.out=FALSE,...)

claraCBI(data,k,usepam=TRUE,diss=inherits(data,"dist"),...)

pamkCBI(data,krange=2:10,k=NULL,criterion="asw", usepam=TRUE,
        scaling=FALSE,diss=inherits(data,"dist"),...)

tclustCBI(data,k,trim=0.05,...)

dbscanCBI(data,eps,MinPts,diss=inherits(data,"dist"),...)

mahalCBI(data,clustercut=0.5,...)

mergenormCBI(data, G=NULL, k=NULL, modelNames=NULL, nnk=0,
                         hcmodel = NULL,
                         Vinv = NULL, mergemethod="bhat",
                         cutoff=0.1,...)

speccCBI(data,k,...)

pdfclustCBI(data,...)



stupidkcentroidsCBI(dmatrix,k,distances=TRUE)

stupidknnCBI(dmatrix,k)

stupidkfnCBI(dmatrix,k)

stupidkavenCBI(dmatrix,k)

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>a numeric matrix. The data
matrix - usually a cases*variables-data matrix. <code>claraCBI</code>,
<code>pamkCBI</code> and <code>dbscanCBI</code> work with an
<code>n*n</code>-dissimilarity matrix as well, see parameter <code>diss</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dmatrix</code></td>
<td>
<p>a squared numerical dissimilarity matrix or a
<code>dist</code>-object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>numeric, usually integer. In most cases, this is the number
of clusters for methods where this is fixed. For <code>hclustCBI</code>
and <code>disthclustCBI</code> see parameter <code>cut</code> below. Some
methods have a <code>k</code> parameter on top of a <code>G</code> or
<code>krange</code> parameter for compatibility; <code>k</code> in these cases
does not have to be specified but if it is, it is always a single
number of clusters and overwrites <code>G</code> and
<code>krange</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scaling</code></td>
<td>
<p>either a logical value or a numeric vector of length
equal to the number of variables. If <code>scaling</code> is a numeric
vector with length equal to the number of variables, then each
variable is divided by the corresponding value from <code>scaling</code>.
If <code>scaling</code> is <code>TRUE</code> then scaling is done by dividing
the (centered) variables by their root-mean-square, and if
<code>scaling</code> is <code>FALSE</code>, no scaling is done before execution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>runs</code></td>
<td>
<p>integer. Number of random initializations from which the
k-means algorithm is started.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>criterion</code></td>
<td>
<p><code>"ch"</code> or <code>"asw"</code>. Decides whether number
of clusters is estimated by the Calinski-Harabasz criterion or by the
average silhouette width.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cut</code></td>
<td>
<p>either "level" or "number". This determines how
<code>cutree</code> is used to obtain a partition from a hierarchy
tree. <code>cut="level"</code> means that the tree is cut at a particular
dissimilarity level, <code>cut="number"</code> means that the tree is cut
in order to obtain a fixed number of clusters. The parameter
<code>k</code> specifies the number of clusters or the dissimilarity
level, depending on <code>cut</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>method for hierarchical clustering, see the
documentation of <code>hclust</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>noisecut</code></td>
<td>
<p>numeric. All clusters of size <code>&lt;=noisecut</code> in the
<code>disthclustCBI</code>/<code>hclustCBI</code>-partition are joined and declared as
noise/outliers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minlevel</code></td>
<td>
<p>integer. <code>minlevel=1</code> means that all clusters in
the tree are given out by <code>hclusttreeCBI</code> or
<code>disthclusttreeCBI</code>, including one-point
clusters (but excluding the cluster with all
points). <code>minlevel=2</code> excludes the one-point clusters.
<code>minlevel=3</code> excludes the two-point cluster which has been
merged first, and increasing the value of <code>minlevel</code> by 1 in
all further steps means that the remaining earliest formed cluster
is excluded.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>G</code></td>
<td>
<p>vector of integers. Number of clusters or numbers of clusters
used by
<code>mclustBIC</code>. If
<code>G</code> has more than one entry, the number of clusters is
estimated by the BIC.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>modelNames</code></td>
<td>
<p>vector of string. Models for covariance matrices,
see documentation of
<code>mclustBIC</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nnk</code></td>
<td>
<p>integer. Tuning constant for
<code>NNclean</code>, which is used to estimate the
initial noise for <code>noisemclustCBI</code> and
<code>distnoisemclustCBI</code>. See parameter <code>k</code> in the
documentation of <code>NNclean</code>. <code>nnk=0</code> means
that no noise component is fitted.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hcmodel</code></td>
<td>
<p>string or <code>NULL</code>. Determines the initialization of
the EM-algorithm for
<code>mclustBIC</code>.
Documented in <code>hc</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Vinv</code></td>
<td>
<p>numeric. See documentation of
<code>mclustBIC</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>summary.out</code></td>
<td>
<p>logical. If <code>TRUE</code>, the result of
<code>summary.mclustBIC</code> is added as component
<code>mclustsummary</code> to the output of <code>noisemclustCBI</code> and
<code>distnoisemclustCBI</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mdsmethod</code></td>
<td>
<p>"classical", "kruskal" or "sammon". Determines the
multidimensional scaling method to compute Euclidean data from a
dissimilarity matrix. See <code>cmdscale</code>,
<code>isoMDS</code> and <code>sammon</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mdsdim</code></td>
<td>
<p>integer. Dimensionality of MDS solution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>points.out</code></td>
<td>
<p>logical. If <code>TRUE</code>, the matrix of MDS points
is added as component
<code>points</code> to the output of <code>noisemclustCBI</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>usepam</code></td>
<td>
<p>logical. If <code>TRUE</code>, the function
<code>pam</code> is used for clustering, otherwise
<code>clara</code>. <code>pam</code> is better,
<code>clara</code> is faster.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>diss</code></td>
<td>
<p>logical. If <code>TRUE</code>, <code>data</code> will be considered as
a dissimilarity matrix. In <code>claraCBI</code>, this requires
<code>usepam=TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>krange</code></td>
<td>
<p>vector of integers. Numbers of clusters to be compared.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trim</code></td>
<td>
<p>numeric between 0 and 1. Proportion of data points
trimmed, i.e., assigned to noise. See <code>tclust</code> in the tclust package.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>numeric. The radius of the neighborhoods to be considered
by <code>dbscan</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MinPts</code></td>
<td>
<p>integer. How many points have to be in a neighborhood so
that a point is considered to be a cluster seed? See documentation
of <code>dbscan</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clustercut</code></td>
<td>
<p>numeric between 0 and 1. If <code>fixmahal</code>
is used for fuzzy clustering, a crisp partition is generated and
points with cluster membership values above <code>clustercut</code> are
considered as members of the corresponding cluster.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mergemethod</code></td>
<td>
<p>method for merging Gaussians, passed on as
<code>method</code> to <code>mergenormals</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cutoff</code></td>
<td>
<p>numeric between 0 and 1, tuning constant for
<code>mergenormals</code>.</p>
</td>
</tr>
</table>
<table>
<tr style="vertical-align: top;">
<td><code>distances</code></td>
<td>
<p>logical (only for <code>stupidkcentroidsCBI</code>). If
<code>FALSE</code>, <code>dmatrix</code> is
interpreted as cases&amp;variables data matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further parameters to be transferred to the original
clustering functions (not required).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>All these functions call clustering methods implemented in R to
cluster data and to provide output in the format required by
<code>clusterboot</code>. Here is a brief overview. For further
details see the help pages of the involved clustering methods.
</p>

<dl>
<dt>kmeansCBI</dt>
<dd>
<p>an interface to the function
<code>kmeansruns</code> calling <code>kmeans</code>
for k-means clustering. (<code>kmeansruns</code> allows the
specification of several random initializations of the
k-means algorithm and estimation of k by the Calinski-Harabasz
index or the average silhouette width.)</p>
</dd>
<dt>hclustCBI</dt>
<dd>
<p>an interface to the function
<code>hclust</code> for agglomerative hierarchical clustering with
noise component (see parameter <code>noisecut</code> above). This
function produces a partition and assumes a cases*variables
matrix as input.</p>
</dd>
<dt>hclusttreeCBI</dt>
<dd>
<p>an interface to the function
<code>hclust</code> for agglomerative hierarchical clustering. This
function gives out all clusters belonging to the hierarchy
(upward from a certain level, see parameter <code>minlevel</code>
above).</p>
</dd>
<dt>disthclustCBI</dt>
<dd>
<p>an interface to the function
<code>hclust</code> for agglomerative hierarchical clustering with
noise component (see parameter <code>noisecut</code> above). This
function produces a partition and assumes a dissimilarity
matrix as input.</p>
</dd>
</dl>
<dl>
<dt>noisemclustCBI</dt>
<dd>
<p>an interface to the function
<code>mclustBIC</code>, for normal mixture model based
clustering. Warning: <code>mclustBIC</code> often
has problems with multiple
points. In <code>clusterboot</code>, it is recommended to use
this together with <code>multipleboot=FALSE</code>.</p>
</dd>	
<dt>distnoisemclustCBI</dt>
<dd>
<p>an interface to the function
<code>mclustBIC</code> for normal mixture model based
clustering. This assumes a dissimilarity matrix as input and
generates a data matrix by multidimensional scaling first.
Warning: <code>mclustBIC</code> often has
problems with multiple
points. In <code>clusterboot</code>, it is recommended to use
this together with <code>multipleboot=FALSE</code>.</p>
</dd>
<dt>claraCBI</dt>
<dd>
<p>an interface to the functions
<code>pam</code> and <code>clara</code>
for partitioning around medoids.</p>
</dd>
<dt>pamkCBI</dt>
<dd>
<p>an interface to the function
<code>pamk</code> calling <code>pam</code> for
partitioning around medoids. The number
of clusters is estimated by the Calinski-Harabasz index or by the
average silhouette width.</p>
</dd>
<dt>tclustCBI</dt>
<dd>
<p>an interface to the function
<code>tclust</code> in the tclust package for trimmed Gaussian 
clustering. This assumes a cases*variables matrix as input.</p>
</dd>
</dl>
<dl>
<dt>dbscanCBI</dt>
<dd>
<p>an interface to the function
<code>dbscan</code> for density based 
clustering.</p>
</dd>
<dt>mahalCBI</dt>
<dd>
<p>an interface to the function
<code>fixmahal</code> for fixed point
clustering. This assumes a cases*variables matrix as input.</p>
</dd>
<dt>mergenormCBI</dt>
<dd>
<p>an interface to the function
<code>mergenormals</code> for clustering by merging Gaussian
mixture components. Unlike <code>mergenormals</code>, <code>mergenormCBI</code>
includes the computation of the initial Gaussian mixture.
This assumes a cases*variables matrix as input.
</p>
</dd>
<dt>speccCBI</dt>
<dd>
<p>an interface to the function
<code>specc</code> for spectral clustering. See
the <code>specc</code> help page for additional tuning
parameters. This assumes a cases*variables matrix as input.</p>
</dd>
<dt>pdfclustCBI</dt>
<dd>
<p>an interface to the function
<code>pdfCluster</code> for density-based clustering. See
the <code>pdfCluster</code> help page for additional tuning
parameters. This assumes a cases*variables matrix as input.</p>
</dd>
</dl>
<dl>
<dt>stupidkcentroidsCBI</dt>
<dd>
<p>an interface to the function
<code>stupidkcentroids</code> for random centroid-based clustering. See
the <code>stupidkcentroids</code> help page. This can have a
distance matrix as well as a cases*variables matrix as input, see
parameter <code>distances</code>.</p>
</dd>
<dt>stupidknnCBI</dt>
<dd>
<p>an interface to the function
<code>stupidknn</code> for random nearest neighbour clustering. See
the <code>stupidknn</code> help page. This assumes a
distance matrix as input.</p>
</dd>
<dt>stupidkfnCBI</dt>
<dd>
<p>an interface to the function
<code>stupidkfn</code> for random farthest neighbour clustering. See
the <code>stupidkfn</code> help page. This assumes a
distance matrix as input.</p>
</dd>
<dt>stupidkavenCBI</dt>
<dd>
<p>an interface to the function
<code>stupidkaven</code> for random average dissimilarity clustering. See
the <code>stupidkaven</code> help page. This assumes a
distance matrix as input.</p>
</dd>
</dl>
<h3>Value</h3>

<p>All interface functions return a list with the following components
(there may be some more, see <code>summary.out</code> and <code>points.out</code>
above):
</p>
<table>
<tr style="vertical-align: top;">
<td><code>result</code></td>
<td>
<p>clustering result, usually a list with the full
output of the clustering method (the precise format doesn't
matter); whatever you want to use later.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nc</code></td>
<td>
<p>number of clusters. If some points don't belong to any
cluster, these are declared "noise". <code>nc</code> includes the
"noise cluster", and there should be another component
<code>nccl</code>, being the number of clusters not including the
noise cluster.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clusterlist</code></td>
<td>
<p>this is a list consisting of a logical vectors
of length of the number of data points (<code>n</code>) for each cluster,
indicating whether a point is a member of this cluster
(<code>TRUE</code>) or not. If a noise cluster is included, it
should always be the last vector in this list.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>partition</code></td>
<td>
<p>an integer vector of length <code>n</code>,
partitioning the data. If the method produces a partition, it
should be the clustering. This component is only used for plots,
so you could do something like <code>rep(1,n)</code> for
non-partitioning methods. If a noise cluster is included,
<code>nc=nccl+1</code> and the noise cluster is cluster no. <code>nc</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clustermethod</code></td>
<td>
<p>a string indicating the clustering method.</p>
</td>
</tr>
</table>
<p>The output of some of the functions has further components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>nccl</code></td>
<td>
<p>see <code>nc</code> above.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nnk</code></td>
<td>
<p>by <code>noisemclustCBI</code> and <code>distnoisemclustCBI</code>,
see above.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initnoise</code></td>
<td>
<p>logical vector, indicating initially estimated noise by
<code>NNclean</code>, called by <code>noisemclustCBI</code>
and <code>distnoisemclustCBI</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>noise</code></td>
<td>
<p>logical. <code>TRUE</code> if points were classified as
noise/outliers by <code>disthclustCBI</code>.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:christian.hennig@unibo.it">christian.hennig@unibo.it</a>
<a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</p>


<h3>See Also</h3>

<p><code>clusterboot</code>, <code>dist</code>,
<code>kmeans</code>, <code>kmeansruns</code>, <code>hclust</code>,
<code>mclustBIC</code>, 
<code>pam</code>,  <code>pamk</code>,
<code>clara</code>,
<code>dbscan</code>,
<code>fixmahal</code>,
<code>tclust</code>, <code>pdfCluster</code>

</p>


<h3>Examples</h3>

<pre><code class="language-R">  options(digits=3)
  set.seed(20000)
  face &lt;- rFace(50,dMoNo=2,dNoEy=0,p=2)
  dbs &lt;- dbscanCBI(face,eps=1.5,MinPts=4)
  dhc &lt;- disthclustCBI(dist(face),method="average",k=1.5,noisecut=2)
  table(dbs$partition,dhc$partition)
  dm &lt;- mergenormCBI(face,G=10,modelNames="EEE",nnk=2)
  dtc &lt;- tclustCBI(face,6,trim=0.1,restr.fact=500)
  table(dm$partition,dtc$partition)

</code></pre>


</div>