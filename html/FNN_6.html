<div class="container">

<table style="width: 100%;"><tr>
<td>KL.divergence</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kullback-Leibler Divergence</h2>

<h3>Description</h3>

<p>Compute Kullback-Leibler divergence.</p>


<h3>Usage</h3>

<pre><code class="language-R">  KL.divergence(X, Y, k = 10, algorithm=c("kd_tree", "cover_tree", "brute"))
  KLx.divergence(X, Y, k = 10, algorithm="kd_tree")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>An input data matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>An input data matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>The maximum number of nearest neighbors to search. The default value
is set to 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>algorithm</code></td>
<td>
<p>nearest neighbor search algorithm.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If <code>p(x)</code> and <code>q(x)</code> are two continuous probability density functions,
then the Kullback-Leibler divergence of <code>q</code> from <code>p</code> is defined as
<code class="reqn">E_p[\log \frac{p(x)}{q(x)}]</code>.
</p>
<p><code>KL.*</code> versions return divergences from <code>C</code> code to <code>R</code> but <code>KLx.*</code> do not.
</p>


<h3>Value</h3>

<p>Return the Kullback-Leibler divergence from <code>X</code> to <code>Y</code>.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>S. Boltz, E. Debreuve and M. Barlaud (2007).
“kNN-based high-dimensional Kullback-Leibler distance for tracking”.
<em>Image Analysis for Multimedia Interactive Services, 2007. WIAMIS '07. Eighth International Workshop on</em>.
</p>
<p>S. Boltz, E. Debreuve and M. Barlaud (2009).
“High-dimensional statistical measure for region-of-interest tracking”.
<em>Trans. Img. Proc.</em>, <b>18</b>:6, 1266–1283.
</p>


<h3>See Also</h3>

<p><code>KL.dist</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">    set.seed(1000)
    X&lt;- rexp(10000, rate=0.2)
    Y&lt;- rexp(10000, rate=0.4)

    KL.divergence(X, Y, k=5)
    #theoretical divergence = log(0.2/0.4)+(0.4/0.2)-1 = 1-log(2) = 0.307
</code></pre>


</div>